{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import lightgbm as lgb\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import gc\n",
        "\n",
        "# ===================================================================\n",
        "# 1. Load and Prepare Data\n",
        "# ===================================================================\n",
        "print(\"Loading data...\")\n",
        "df = pd.read_csv(\"/content/normalized_features.csv\")\n",
        "\n",
        "df['Timestamp'] = pd.to_datetime(df['Timestamp'])\n",
        "df = df.sort_values(by=['Location', 'Timestamp']).reset_index(drop=True)\n",
        "\n",
        "# ==> CHANGE 1: Apply log transformation to the target variable 'out'\n",
        "# We create a new column for the transformed target to keep the original for validation.\n",
        "df['out_transformed'] = np.log1p(df['out'])\n",
        "\n",
        "le = LabelEncoder()\n",
        "df['Location_encoded'] = le.fit_transform(df['Location'])\n",
        "\n",
        "\n",
        "# ===================================================================\n",
        "# 2. Feature Engineering for Time-Series\n",
        "# ===================================================================\n",
        "print(\"Engineering features...\")\n",
        "\n",
        "def create_time_series_features(df_in):\n",
        "    df_out = df_in.copy()\n",
        "    features_to_lag = ['out', 't2m', 'gust', 'cape', 'prate']\n",
        "    lag_periods = [1, 2, 3, 24, 48]\n",
        "\n",
        "    for feature in features_to_lag:\n",
        "        for lag in lag_periods:\n",
        "            df_out[f'{feature}_lag_{lag}'] = df_out.groupby('Location')[feature].shift(lag)\n",
        "\n",
        "    df_out['gust_roll_max_3h'] = df_out.groupby('Location')['gust'].transform(\n",
        "        lambda x: x.shift(1).rolling(window=3).max()\n",
        "    )\n",
        "    df_out['hour'] = df_out['Timestamp'].dt.hour\n",
        "    df_out['dayofweek'] = df_out['Timestamp'].dt.dayofweek\n",
        "    return df_out\n",
        "\n",
        "df_features = create_time_series_features(df)\n",
        "\n",
        "features = [col for col in df_features.columns if col not in [\n",
        "    'out', 'out_transformed', 'Timestamp', 'Location'\n",
        "]]\n",
        "\n",
        "\n",
        "# ===================================================================\n",
        "# 3. Create a Time-Based Validation Split\n",
        "# ===================================================================\n",
        "print(\"Creating time-based train/validation split...\")\n",
        "validation_start_date = df_features['Timestamp'].max() - pd.Timedelta(days=7)\n",
        "\n",
        "train_df = df_features[df_features['Timestamp'] < validation_start_date].copy()\n",
        "val_df = df_features[df_features['Timestamp'] >= validation_start_date].copy()\n",
        "\n",
        "train_df.dropna(inplace=True)\n",
        "\n",
        "\n",
        "# ===================================================================\n",
        "# 4. Train Models with Direct Forecasting Strategy\n",
        "# ===================================================================\n",
        "print(\"Starting model training...\")\n",
        "MAX_HORIZON = 48\n",
        "models = {}\n",
        "\n",
        "for h in range(1, MAX_HORIZON + 1):\n",
        "    print(f\"\\n===== Training model for horizon t+{h} =====\")\n",
        "\n",
        "    # Use the TRANSFORMED 'out' column as the target\n",
        "    y_train_h = train_df.groupby('Location')['out_transformed'].shift(-h)\n",
        "    X_train_h = train_df[features]\n",
        "\n",
        "    valid_indices = ~y_train_h.isna()\n",
        "    X_train_h, y_train_h = X_train_h[valid_indices], y_train_h[valid_indices]\n",
        "\n",
        "    # Create validation set for this horizon\n",
        "    X_val_h = val_df[features]\n",
        "    y_val_h = val_df.groupby('Location')['out_transformed'].shift(-h).dropna()\n",
        "    X_val_h = X_val_h.loc[y_val_h.index]\n",
        "\n",
        "    # ==> CHANGE 2: Objective is 'regression_l2' (RMSE)\n",
        "    lgbm = lgb.LGBMRegressor(\n",
        "        objective='regression_l2',\n",
        "        n_estimators=2000,\n",
        "        learning_rate=0.01,\n",
        "        num_leaves=40,\n",
        "        random_state=42,\n",
        "        n_jobs=-1,\n",
        "        colsample_bytree=0.7,\n",
        "        subsample=0.7,\n",
        "    )\n",
        "\n",
        "    lgbm.fit(\n",
        "        X_train_h, y_train_h,\n",
        "        eval_set=[(X_val_h, y_val_h)],\n",
        "        eval_metric='rmse',\n",
        "        # ==> CHANGE 3: Callback will now print the loss as it improves\n",
        "        callbacks=[lgb.early_stopping(100, verbose=True)]\n",
        "    )\n",
        "\n",
        "    models[h] = lgbm\n",
        "    gc.collect()\n",
        "\n",
        "print(\"\\nModel training complete.\")\n",
        "\n",
        "\n",
        "# ===================================================================\n",
        "# 5. Generate and Evaluate Predictions\n",
        "# ===================================================================\n",
        "print(\"Generating and evaluating predictions on the validation set...\")\n",
        "\n",
        "validation_predictions = []\n",
        "for loc_id in val_df['Location'].unique():\n",
        "    loc_df = val_df[val_df['Location'] == loc_id].copy()\n",
        "\n",
        "    # Use the features from the first timestamp of the validation set for this location\n",
        "    X_pred_loc = loc_df[features].iloc[[0]]\n",
        "\n",
        "    for h in range(1, MAX_HORIZON + 1):\n",
        "        if h > len(loc_df): continue # Cannot validate if horizon is longer than remaining data\n",
        "\n",
        "        prediction_transformed = models[h].predict(X_pred_loc)[0]\n",
        "\n",
        "        # ==> CHANGE 4: Reverse the transformation using expm1\n",
        "        prediction_original_scale = np.expm1(prediction_transformed)\n",
        "        prediction_original_scale = max(0, prediction_original_scale)\n",
        "\n",
        "        # Get the true value from the original 'out' column\n",
        "        true_value = loc_df['out'].iloc[h-1]\n",
        "\n",
        "        validation_predictions.append({\n",
        "            'pred': prediction_original_scale,\n",
        "            'true': true_value\n",
        "        })\n",
        "\n",
        "final_preds_df = pd.DataFrame(validation_predictions)\n",
        "rmse = np.sqrt(mean_squared_error(final_preds_df['true'], final_preds_df['pred']))\n",
        "print(f\"\\nâœ… Final Validation RMSE on original scale: {rmse:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_73bNaFzvxy0",
        "outputId": "db49e3f0-68b0-4166-d54a-ad153506f6fe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading data...\n",
            "Engineering features...\n",
            "Creating time-based train/validation split...\n",
            "Starting model training...\n",
            "\n",
            "===== Training model for horizon t+1 =====\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[691]\tvalid_0's rmse: 1.10068\tvalid_0's l2: 1.21149\n",
            "\n",
            "===== Training model for horizon t+2 =====\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[760]\tvalid_0's rmse: 1.22217\tvalid_0's l2: 1.49371\n",
            "\n",
            "===== Training model for horizon t+3 =====\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[606]\tvalid_0's rmse: 1.30131\tvalid_0's l2: 1.69341\n",
            "\n",
            "===== Training model for horizon t+4 =====\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[869]\tvalid_0's rmse: 1.35171\tvalid_0's l2: 1.82711\n",
            "\n",
            "===== Training model for horizon t+5 =====\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[1179]\tvalid_0's rmse: 1.40238\tvalid_0's l2: 1.96667\n",
            "\n",
            "===== Training model for horizon t+6 =====\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[1032]\tvalid_0's rmse: 1.42987\tvalid_0's l2: 2.04453\n",
            "\n",
            "===== Training model for horizon t+7 =====\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[1334]\tvalid_0's rmse: 1.45727\tvalid_0's l2: 2.12365\n",
            "\n",
            "===== Training model for horizon t+8 =====\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[1800]\tvalid_0's rmse: 1.47895\tvalid_0's l2: 2.1873\n",
            "\n",
            "===== Training model for horizon t+9 =====\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[1999]\tvalid_0's rmse: 1.49982\tvalid_0's l2: 2.24947\n",
            "\n",
            "===== Training model for horizon t+10 =====\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[1673]\tvalid_0's rmse: 1.52467\tvalid_0's l2: 2.32462\n",
            "\n",
            "===== Training model for horizon t+11 =====\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[1136]\tvalid_0's rmse: 1.55489\tvalid_0's l2: 2.41769\n",
            "\n",
            "===== Training model for horizon t+12 =====\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[1999]\tvalid_0's rmse: 1.56447\tvalid_0's l2: 2.44755\n",
            "\n",
            "===== Training model for horizon t+13 =====\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[1400]\tvalid_0's rmse: 1.59161\tvalid_0's l2: 2.53323\n",
            "\n",
            "===== Training model for horizon t+14 =====\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[1803]\tvalid_0's rmse: 1.59811\tvalid_0's l2: 2.55394\n",
            "\n",
            "===== Training model for horizon t+15 =====\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[980]\tvalid_0's rmse: 1.62448\tvalid_0's l2: 2.63892\n",
            "\n",
            "===== Training model for horizon t+16 =====\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[1868]\tvalid_0's rmse: 1.62253\tvalid_0's l2: 2.6326\n",
            "\n",
            "===== Training model for horizon t+17 =====\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[993]\tvalid_0's rmse: 1.64574\tvalid_0's l2: 2.70844\n",
            "\n",
            "===== Training model for horizon t+18 =====\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[1968]\tvalid_0's rmse: 1.64071\tvalid_0's l2: 2.69193\n",
            "\n",
            "===== Training model for horizon t+19 =====\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[1318]\tvalid_0's rmse: 1.66112\tvalid_0's l2: 2.75933\n",
            "\n",
            "===== Training model for horizon t+20 =====\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[1247]\tvalid_0's rmse: 1.66347\tvalid_0's l2: 2.76715\n",
            "\n",
            "===== Training model for horizon t+21 =====\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[1983]\tvalid_0's rmse: 1.66338\tvalid_0's l2: 2.76685\n",
            "\n",
            "===== Training model for horizon t+22 =====\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[2000]\tvalid_0's rmse: 1.66915\tvalid_0's l2: 2.78607\n",
            "\n",
            "===== Training model for horizon t+23 =====\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[1993]\tvalid_0's rmse: 1.67223\tvalid_0's l2: 2.79634\n",
            "\n",
            "===== Training model for horizon t+24 =====\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[1913]\tvalid_0's rmse: 1.67956\tvalid_0's l2: 2.82093\n",
            "\n",
            "===== Training model for horizon t+25 =====\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[1848]\tvalid_0's rmse: 1.69296\tvalid_0's l2: 2.8661\n",
            "\n",
            "===== Training model for horizon t+26 =====\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[1991]\tvalid_0's rmse: 1.69976\tvalid_0's l2: 2.88919\n",
            "\n",
            "===== Training model for horizon t+27 =====\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[1853]\tvalid_0's rmse: 1.72964\tvalid_0's l2: 2.99165\n",
            "\n",
            "===== Training model for horizon t+28 =====\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[1993]\tvalid_0's rmse: 1.72707\tvalid_0's l2: 2.98276\n",
            "\n",
            "===== Training model for horizon t+29 =====\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[1839]\tvalid_0's rmse: 1.75221\tvalid_0's l2: 3.07023\n",
            "\n",
            "===== Training model for horizon t+30 =====\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[1364]\tvalid_0's rmse: 1.77157\tvalid_0's l2: 3.13845\n",
            "\n",
            "===== Training model for horizon t+31 =====\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[1109]\tvalid_0's rmse: 1.78421\tvalid_0's l2: 3.18341\n",
            "\n",
            "===== Training model for horizon t+32 =====\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[621]\tvalid_0's rmse: 1.8018\tvalid_0's l2: 3.24649\n",
            "\n",
            "===== Training model for horizon t+33 =====\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[715]\tvalid_0's rmse: 1.80519\tvalid_0's l2: 3.25871\n",
            "\n",
            "===== Training model for horizon t+34 =====\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[941]\tvalid_0's rmse: 1.80865\tvalid_0's l2: 3.2712\n",
            "\n",
            "===== Training model for horizon t+35 =====\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[597]\tvalid_0's rmse: 1.82627\tvalid_0's l2: 3.33527\n",
            "\n",
            "===== Training model for horizon t+36 =====\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[790]\tvalid_0's rmse: 1.82949\tvalid_0's l2: 3.34702\n",
            "\n",
            "===== Training model for horizon t+37 =====\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[600]\tvalid_0's rmse: 1.83864\tvalid_0's l2: 3.38061\n",
            "\n",
            "===== Training model for horizon t+38 =====\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[863]\tvalid_0's rmse: 1.83796\tvalid_0's l2: 3.37811\n",
            "\n",
            "===== Training model for horizon t+39 =====\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[825]\tvalid_0's rmse: 1.84796\tvalid_0's l2: 3.41497\n",
            "\n",
            "===== Training model for horizon t+40 =====\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[784]\tvalid_0's rmse: 1.85501\tvalid_0's l2: 3.44106\n",
            "\n",
            "===== Training model for horizon t+41 =====\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[789]\tvalid_0's rmse: 1.86775\tvalid_0's l2: 3.4885\n",
            "\n",
            "===== Training model for horizon t+42 =====\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[842]\tvalid_0's rmse: 1.88163\tvalid_0's l2: 3.54053\n",
            "\n",
            "===== Training model for horizon t+43 =====\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[926]\tvalid_0's rmse: 1.87772\tvalid_0's l2: 3.52585\n",
            "\n",
            "===== Training model for horizon t+44 =====\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[745]\tvalid_0's rmse: 1.89023\tvalid_0's l2: 3.57298\n",
            "\n",
            "===== Training model for horizon t+45 =====\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[720]\tvalid_0's rmse: 1.89578\tvalid_0's l2: 3.59397\n",
            "\n",
            "===== Training model for horizon t+46 =====\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[625]\tvalid_0's rmse: 1.89511\tvalid_0's l2: 3.59143\n",
            "\n",
            "===== Training model for horizon t+47 =====\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[743]\tvalid_0's rmse: 1.88997\tvalid_0's l2: 3.57198\n",
            "\n",
            "===== Training model for horizon t+48 =====\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[551]\tvalid_0's rmse: 1.89327\tvalid_0's l2: 3.58449\n",
            "\n",
            "Model training complete.\n",
            "Generating and evaluating predictions on the validation set...\n",
            "\n",
            "âœ… Final Validation RMSE on original scale: 193.9203\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ===================================================================\n",
        "# 4. Generate Predictions for Submission\n",
        "# ===================================================================\n",
        "print(\"Generating predictions for submission files...\")\n",
        "\n",
        "# Get the last row of data for each location from the full feature-engineered dataframe.\n",
        "# This row contains the most recent lagged features needed for prediction.\n",
        "last_known_data = df_features.groupby('Location').last().reset_index()\n",
        "\n",
        "# Load the submission templates to get the required timestamps and locations.\n",
        "sub_template_24h = pd.read_csv('/content/submission_template_24h.csv')\n",
        "sub_template_48h = pd.read_csv('/content/submission_template_48h.csv')\n",
        "\n",
        "sub_template_24h['timestamp'] = pd.to_datetime(sub_template_24h['timestamp'])\n",
        "sub_template_48h['timestamp'] = pd.to_datetime(sub_template_48h['timestamp'])\n",
        "\n",
        "\n",
        "all_predictions = []\n",
        "\n",
        "for loc_encoded, loc_id in enumerate(le.classes_):\n",
        "    # Get the feature set for the current location\n",
        "    X_pred_loc = last_known_data[last_known_data['Location'] == loc_id][features]\n",
        "\n",
        "    # If a location has no data, we can't predict\n",
        "    if X_pred_loc.empty:\n",
        "        continue\n",
        "\n",
        "    for h in range(1, MAX_HORIZON + 1):\n",
        "        # Predict using the model for horizon 'h'\n",
        "        prediction = models[h].predict(X_pred_loc)[0]\n",
        "\n",
        "        # Ensure prediction is non-negative\n",
        "        prediction = max(0, prediction)\n",
        "\n",
        "        # The prediction timestamp is h hours after the last known timestamp\n",
        "        pred_timestamp = last_known_data.loc[last_known_data['Location'] == loc_id, 'Timestamp'].iloc[0] + pd.Timedelta(hours=h)\n",
        "\n",
        "        all_predictions.append({\n",
        "            'timestamp': pred_timestamp,\n",
        "            'location': loc_id,\n",
        "            'pred': prediction\n",
        "        })\n",
        "\n",
        "predictions_df = pd.DataFrame(all_predictions)\n",
        "\n",
        "\n",
        "# ===================================================================\n",
        "# 5. Create and Save Submission Files\n",
        "# ===================================================================\n",
        "print(\"Saving submission files...\")\n",
        "\n",
        "# Create 24-hour submission file\n",
        "submission_24h = sub_template_24h[['timestamp', 'location']].copy()\n",
        "submission_24h = submission_24h.merge(predictions_df, on=['timestamp', 'location'], how='left')\n",
        "submission_24h['pred'] = submission_24h['pred'].fillna(0) # Fill any misses with 0\n",
        "\n",
        "# Create 48-hour submission file\n",
        "submission_48h = sub_template_48h[['timestamp', 'location']].copy()\n",
        "submission_48h = submission_48h.merge(predictions_df, on=['timestamp', 'location'], how='left')\n",
        "submission_48h['pred'] = submission_48h['pred'].fillna(0) # Fill any misses with 0\n",
        "\n",
        "\n",
        "# Save to CSV in the specified format without the index\n",
        "submission_24h.to_csv('submission_24h.csv', index=False)\n",
        "submission_48h.to_csv('submission_48h.csv', index=False)\n",
        "\n",
        "print(f\"âœ… Successfully created submission_24h.csv and submission_48h.csv\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oIkZwzZq7WD4",
        "outputId": "c31c6a38-1c13-4ad5-e673-3fcdfb5bb62b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating predictions for submission files...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1711972479.py:14: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
            "  sub_template_24h['timestamp'] = pd.to_datetime(sub_template_24h['timestamp'])\n",
            "/tmp/ipython-input-1711972479.py:15: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
            "  sub_template_48h['timestamp'] = pd.to_datetime(sub_template_48h['timestamp'])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving submission files...\n",
            "âœ… Successfully created submission_24h.csv and submission_48h.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib\n",
        "\n",
        "joblib.dump(models, 'lgbm_direct_forecasting_models.joblib')\n",
        "print(\"âœ… Models have been saved to lgbm_direct_forecasting_models.joblib\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AJACf4qY8T8P",
        "outputId": "345faba8-dcf9-47aa-e200-7647a795767e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Models have been saved to lgbm_direct_forecasting_models.joblib\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "TRYING TO IMPROVE RMSE EVEN FURTHER"
      ],
      "metadata": {
        "id": "CV5Fi8EV_yZe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import lightgbm as lgb\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import gc\n",
        "\n",
        "# ===================================================================\n",
        "# 1. Load and Prepare Data\n",
        "# ===================================================================\n",
        "print(\"Loading data...\")\n",
        "df = pd.read_csv(\"/content/normalized_features.csv\")\n",
        "\n",
        "df['Timestamp'] = pd.to_datetime(df['Timestamp'])\n",
        "df = df.sort_values(by=['Location', 'Timestamp']).reset_index(drop=True)\n",
        "\n",
        "# ==> CHANGE 1: Apply log transformation to the target variable 'out'\n",
        "# We create a new column for the transformed target to keep the original for validation.\n",
        "df['out_transformed'] = np.log1p(df['out'])\n",
        "\n",
        "le = LabelEncoder()\n",
        "df['Location_encoded'] = le.fit_transform(df['Location'])\n",
        "\n",
        "\n",
        "# ===================================================================\n",
        "# 2. Feature Engineering for Time-Series\n",
        "# ===================================================================\n",
        "print(\"Engineering features...\")\n",
        "\n",
        "def create_time_series_features(df_in):\n",
        "    df_out = df_in.copy()\n",
        "    features_to_lag = ['out', 't2m', 'gust', 'cape', 'prate']\n",
        "    lag_periods = [1, 2, 3, 24, 48]\n",
        "\n",
        "    for feature in features_to_lag:\n",
        "        for lag in lag_periods:\n",
        "            df_out[f'{feature}_lag_{lag}'] = df_out.groupby('Location')[feature].shift(lag)\n",
        "\n",
        "    df_out['gust_roll_max_3h'] = df_out.groupby('Location')['gust'].transform(\n",
        "        lambda x: x.shift(1).rolling(window=3).max()\n",
        "    )\n",
        "    df_out['hour'] = df_out['Timestamp'].dt.hour\n",
        "    df_out['dayofweek'] = df_out['Timestamp'].dt.dayofweek\n",
        "    return df_out\n",
        "\n",
        "df_features = create_time_series_features(df)\n",
        "\n",
        "features = [col for col in df_features.columns if col not in [\n",
        "    'out', 'out_transformed', 'Timestamp', 'Location'\n",
        "]]\n",
        "\n",
        "\n",
        "# ===================================================================\n",
        "# 3. Create a Time-Based Validation Split\n",
        "# ===================================================================\n",
        "print(\"Creating time-based train/validation split...\")\n",
        "validation_start_date = df_features['Timestamp'].max() - pd.Timedelta(days=7)\n",
        "\n",
        "train_df = df_features[df_features['Timestamp'] < validation_start_date].copy()\n",
        "val_df = df_features[df_features['Timestamp'] >= validation_start_date].copy()\n",
        "\n",
        "train_df.dropna(inplace=True)\n",
        "\n",
        "\n",
        "# ===================================================================\n",
        "# 4. Train Models with Direct Forecasting Strategy\n",
        "# ===================================================================\n",
        "print(\"Starting model training...\")\n",
        "MAX_HORIZON = 48\n",
        "models = {}\n",
        "\n",
        "for h in range(1, MAX_HORIZON + 1):\n",
        "    print(f\"\\n===== Training model for horizon t+{h} =====\")\n",
        "\n",
        "    # Use the TRANSFORMED 'out' column as the target\n",
        "    y_train_h = train_df.groupby('Location')['out_transformed'].shift(-h)\n",
        "    X_train_h = train_df[features]\n",
        "\n",
        "    valid_indices = ~y_train_h.isna()\n",
        "    X_train_h, y_train_h = X_train_h[valid_indices], y_train_h[valid_indices]\n",
        "\n",
        "    # Create validation set for this horizon\n",
        "    X_val_h = val_df[features]\n",
        "    y_val_h = val_df.groupby('Location')['out_transformed'].shift(-h).dropna()\n",
        "    X_val_h = X_val_h.loc[y_val_h.index]\n",
        "\n",
        "    # ==> CHANGE 2: Objective is 'regression_l2' (RMSE)\n",
        "    lgbm = lgb.LGBMRegressor(\n",
        "        objective='regression_l2',\n",
        "        n_estimators=3000,\n",
        "        learning_rate=0.01,\n",
        "        num_leaves=60,\n",
        "        random_state=42,\n",
        "        n_jobs=-1,\n",
        "        colsample_bytree=0.7,\n",
        "        subsample=0.6,\n",
        "    )\n",
        "\n",
        "    lgbm.fit(\n",
        "        X_train_h, y_train_h,\n",
        "        eval_set=[(X_val_h, y_val_h)],\n",
        "        eval_metric='rmse',\n",
        "        # ==> CHANGE 3: Callback will now print the loss as it improves\n",
        "        callbacks=[lgb.early_stopping(100, verbose=True)]\n",
        "    )\n",
        "\n",
        "    models[h] = lgbm\n",
        "    gc.collect()\n",
        "\n",
        "print(\"\\nModel training complete.\")\n",
        "\n",
        "\n",
        "# ===================================================================\n",
        "# 5. Generate and Evaluate Predictions\n",
        "# ===================================================================\n",
        "print(\"Generating and evaluating predictions on the validation set...\")\n",
        "\n",
        "validation_predictions = []\n",
        "for loc_id in val_df['Location'].unique():\n",
        "    loc_df = val_df[val_df['Location'] == loc_id].copy()\n",
        "\n",
        "    # Use the features from the first timestamp of the validation set for this location\n",
        "    X_pred_loc = loc_df[features].iloc[[0]]\n",
        "\n",
        "    for h in range(1, MAX_HORIZON + 1):\n",
        "        if h > len(loc_df): continue # Cannot validate if horizon is longer than remaining data\n",
        "\n",
        "        prediction_transformed = models[h].predict(X_pred_loc)[0]\n",
        "\n",
        "        # ==> CHANGE 4: Reverse the transformation using expm1\n",
        "        prediction_original_scale = np.expm1(prediction_transformed)\n",
        "        prediction_original_scale = max(0, prediction_original_scale)\n",
        "\n",
        "        # Get the true value from the original 'out' column\n",
        "        true_value = loc_df['out'].iloc[h-1]\n",
        "\n",
        "        validation_predictions.append({\n",
        "            'pred': prediction_original_scale,\n",
        "            'true': true_value\n",
        "        })\n",
        "\n",
        "final_preds_df = pd.DataFrame(validation_predictions)\n",
        "rmse = np.sqrt(mean_squared_error(final_preds_df['true'], final_preds_df['pred']))\n",
        "print(f\"\\nâœ… Final Validation RMSE on original scale: {rmse:.4f}\")"
      ],
      "metadata": {
        "id": "I2n232ZU8_bg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5b47b974-15ad-4d1f-9f49-a225f75f89d1"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading data...\n",
            "Engineering features...\n",
            "Creating time-based train/validation split...\n",
            "Starting model training...\n",
            "\n",
            "===== Training model for horizon t+1 =====\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.081791 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 24943\n",
            "[LightGBM] [Info] Number of data points in the train set: 161269, number of used features: 116\n",
            "[LightGBM] [Info] Start training from score 0.673712\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[679]\tvalid_0's rmse: 1.10282\tvalid_0's l2: 1.21621\n",
            "\n",
            "===== Training model for horizon t+2 =====\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.063631 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 24943\n",
            "[LightGBM] [Info] Number of data points in the train set: 161186, number of used features: 116\n",
            "[LightGBM] [Info] Start training from score 0.673603\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[670]\tvalid_0's rmse: 1.2213\tvalid_0's l2: 1.49157\n",
            "\n",
            "===== Training model for horizon t+3 =====\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.058132 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 24943\n",
            "[LightGBM] [Info] Number of data points in the train set: 161103, number of used features: 116\n",
            "[LightGBM] [Info] Start training from score 0.673526\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[692]\tvalid_0's rmse: 1.29782\tvalid_0's l2: 1.68434\n",
            "\n",
            "===== Training model for horizon t+4 =====\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.053455 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 24943\n",
            "[LightGBM] [Info] Number of data points in the train set: 161020, number of used features: 116\n",
            "[LightGBM] [Info] Start training from score 0.673511\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[753]\tvalid_0's rmse: 1.3515\tvalid_0's l2: 1.82654\n",
            "\n",
            "===== Training model for horizon t+5 =====\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.060827 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 24944\n",
            "[LightGBM] [Info] Number of data points in the train set: 160937, number of used features: 116\n",
            "[LightGBM] [Info] Start training from score 0.673573\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[669]\tvalid_0's rmse: 1.40493\tvalid_0's l2: 1.97382\n",
            "\n",
            "===== Training model for horizon t+6 =====\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.059267 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 24944\n",
            "[LightGBM] [Info] Number of data points in the train set: 160854, number of used features: 116\n",
            "[LightGBM] [Info] Start training from score 0.673632\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[993]\tvalid_0's rmse: 1.42992\tvalid_0's l2: 2.04468\n",
            "\n",
            "===== Training model for horizon t+7 =====\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.052856 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 24945\n",
            "[LightGBM] [Info] Number of data points in the train set: 160771, number of used features: 116\n",
            "[LightGBM] [Info] Start training from score 0.673714\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[894]\tvalid_0's rmse: 1.45811\tvalid_0's l2: 2.12609\n",
            "\n",
            "===== Training model for horizon t+8 =====\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.053156 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 24943\n",
            "[LightGBM] [Info] Number of data points in the train set: 160688, number of used features: 115\n",
            "[LightGBM] [Info] Start training from score 0.673822\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[1697]\tvalid_0's rmse: 1.47817\tvalid_0's l2: 2.18498\n",
            "\n",
            "===== Training model for horizon t+9 =====\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.056061 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 24944\n",
            "[LightGBM] [Info] Number of data points in the train set: 160605, number of used features: 115\n",
            "[LightGBM] [Info] Start training from score 0.673929\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[1363]\tvalid_0's rmse: 1.5026\tvalid_0's l2: 2.2578\n",
            "\n",
            "===== Training model for horizon t+10 =====\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.061123 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 24944\n",
            "[LightGBM] [Info] Number of data points in the train set: 160522, number of used features: 115\n",
            "[LightGBM] [Info] Start training from score 0.674038\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[2072]\tvalid_0's rmse: 1.51863\tvalid_0's l2: 2.30625\n",
            "\n",
            "===== Training model for horizon t+11 =====\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.056366 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 24943\n",
            "[LightGBM] [Info] Number of data points in the train set: 160439, number of used features: 115\n",
            "[LightGBM] [Info] Start training from score 0.674150\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[2500]\tvalid_0's rmse: 1.54011\tvalid_0's l2: 2.37194\n",
            "\n",
            "===== Training model for horizon t+12 =====\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.055712 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 24942\n",
            "[LightGBM] [Info] Number of data points in the train set: 160356, number of used features: 115\n",
            "[LightGBM] [Info] Start training from score 0.674208\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[1860]\tvalid_0's rmse: 1.55777\tvalid_0's l2: 2.42666\n",
            "\n",
            "===== Training model for horizon t+13 =====\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.070689 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 24942\n",
            "[LightGBM] [Info] Number of data points in the train set: 160273, number of used features: 115\n",
            "[LightGBM] [Info] Start training from score 0.674205\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[1880]\tvalid_0's rmse: 1.58585\tvalid_0's l2: 2.51492\n",
            "\n",
            "===== Training model for horizon t+14 =====\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.025668 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 24943\n",
            "[LightGBM] [Info] Number of data points in the train set: 160190, number of used features: 115\n",
            "[LightGBM] [Info] Start training from score 0.674162\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[1486]\tvalid_0's rmse: 1.60118\tvalid_0's l2: 2.56379\n",
            "\n",
            "===== Training model for horizon t+15 =====\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.022735 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 24943\n",
            "[LightGBM] [Info] Number of data points in the train set: 160107, number of used features: 115\n",
            "[LightGBM] [Info] Start training from score 0.674143\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[865]\tvalid_0's rmse: 1.62718\tvalid_0's l2: 2.64771\n",
            "\n",
            "===== Training model for horizon t+16 =====\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.056262 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 24943\n",
            "[LightGBM] [Info] Number of data points in the train set: 160024, number of used features: 115\n",
            "[LightGBM] [Info] Start training from score 0.674059\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[862]\tvalid_0's rmse: 1.63729\tvalid_0's l2: 2.68072\n",
            "\n",
            "===== Training model for horizon t+17 =====\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.060547 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 24943\n",
            "[LightGBM] [Info] Number of data points in the train set: 159941, number of used features: 115\n",
            "[LightGBM] [Info] Start training from score 0.673907\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[1021]\tvalid_0's rmse: 1.63971\tvalid_0's l2: 2.68864\n",
            "\n",
            "===== Training model for horizon t+18 =====\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.060912 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 24943\n",
            "[LightGBM] [Info] Number of data points in the train set: 159858, number of used features: 115\n",
            "[LightGBM] [Info] Start training from score 0.673822\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[945]\tvalid_0's rmse: 1.64787\tvalid_0's l2: 2.71546\n",
            "\n",
            "===== Training model for horizon t+19 =====\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.057228 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 24943\n",
            "[LightGBM] [Info] Number of data points in the train set: 159775, number of used features: 115\n",
            "[LightGBM] [Info] Start training from score 0.673763\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[1291]\tvalid_0's rmse: 1.6579\tvalid_0's l2: 2.74865\n",
            "\n",
            "===== Training model for horizon t+20 =====\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.055675 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 24944\n",
            "[LightGBM] [Info] Number of data points in the train set: 159692, number of used features: 115\n",
            "[LightGBM] [Info] Start training from score 0.673688\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[2257]\tvalid_0's rmse: 1.65018\tvalid_0's l2: 2.7231\n",
            "\n",
            "===== Training model for horizon t+21 =====\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.063063 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 24944\n",
            "[LightGBM] [Info] Number of data points in the train set: 159609, number of used features: 115\n",
            "[LightGBM] [Info] Start training from score 0.673601\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[2875]\tvalid_0's rmse: 1.65281\tvalid_0's l2: 2.73177\n",
            "\n",
            "===== Training model for horizon t+22 =====\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.060031 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 24944\n",
            "[LightGBM] [Info] Number of data points in the train set: 159526, number of used features: 115\n",
            "[LightGBM] [Info] Start training from score 0.673524\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[2881]\tvalid_0's rmse: 1.66059\tvalid_0's l2: 2.75757\n",
            "\n",
            "===== Training model for horizon t+23 =====\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.058237 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 24944\n",
            "[LightGBM] [Info] Number of data points in the train set: 159443, number of used features: 115\n",
            "[LightGBM] [Info] Start training from score 0.673491\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[2962]\tvalid_0's rmse: 1.65752\tvalid_0's l2: 2.74739\n",
            "\n",
            "===== Training model for horizon t+24 =====\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.066664 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 24944\n",
            "[LightGBM] [Info] Number of data points in the train set: 159360, number of used features: 115\n",
            "[LightGBM] [Info] Start training from score 0.673524\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[2723]\tvalid_0's rmse: 1.66934\tvalid_0's l2: 2.78671\n",
            "\n",
            "===== Training model for horizon t+25 =====\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.060033 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 24944\n",
            "[LightGBM] [Info] Number of data points in the train set: 159277, number of used features: 115\n",
            "[LightGBM] [Info] Start training from score 0.673578\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[2998]\tvalid_0's rmse: 1.67804\tvalid_0's l2: 2.8158\n",
            "\n",
            "===== Training model for horizon t+26 =====\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.050811 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 24944\n",
            "[LightGBM] [Info] Number of data points in the train set: 159194, number of used features: 115\n",
            "[LightGBM] [Info] Start training from score 0.673674\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[3000]\tvalid_0's rmse: 1.69326\tvalid_0's l2: 2.86712\n",
            "\n",
            "===== Training model for horizon t+27 =====\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.050424 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 24944\n",
            "[LightGBM] [Info] Number of data points in the train set: 159111, number of used features: 115\n",
            "[LightGBM] [Info] Start training from score 0.673770\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[1601]\tvalid_0's rmse: 1.72546\tvalid_0's l2: 2.97721\n",
            "\n",
            "===== Training model for horizon t+28 =====\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.058280 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 24944\n",
            "[LightGBM] [Info] Number of data points in the train set: 159028, number of used features: 115\n",
            "[LightGBM] [Info] Start training from score 0.673825\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[725]\tvalid_0's rmse: 1.7476\tvalid_0's l2: 3.0541\n",
            "\n",
            "===== Training model for horizon t+29 =====\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.060275 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 24944\n",
            "[LightGBM] [Info] Number of data points in the train set: 158945, number of used features: 115\n",
            "[LightGBM] [Info] Start training from score 0.673869\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[1884]\tvalid_0's rmse: 1.74879\tvalid_0's l2: 3.05826\n",
            "\n",
            "===== Training model for horizon t+30 =====\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.019525 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 24944\n",
            "[LightGBM] [Info] Number of data points in the train set: 158862, number of used features: 115\n",
            "[LightGBM] [Info] Start training from score 0.673883\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[938]\tvalid_0's rmse: 1.76876\tvalid_0's l2: 3.12851\n",
            "\n",
            "===== Training model for horizon t+31 =====\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.076040 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 24944\n",
            "[LightGBM] [Info] Number of data points in the train set: 158779, number of used features: 115\n",
            "[LightGBM] [Info] Start training from score 0.673875\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[794]\tvalid_0's rmse: 1.78157\tvalid_0's l2: 3.17399\n",
            "\n",
            "===== Training model for horizon t+32 =====\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.057039 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 24944\n",
            "[LightGBM] [Info] Number of data points in the train set: 158696, number of used features: 115\n",
            "[LightGBM] [Info] Start training from score 0.673844\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[542]\tvalid_0's rmse: 1.79931\tvalid_0's l2: 3.23751\n",
            "\n",
            "===== Training model for horizon t+33 =====\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.050379 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 24944\n",
            "[LightGBM] [Info] Number of data points in the train set: 158613, number of used features: 115\n",
            "[LightGBM] [Info] Start training from score 0.673792\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[614]\tvalid_0's rmse: 1.80391\tvalid_0's l2: 3.25408\n",
            "\n",
            "===== Training model for horizon t+34 =====\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.051607 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 24944\n",
            "[LightGBM] [Info] Number of data points in the train set: 158530, number of used features: 115\n",
            "[LightGBM] [Info] Start training from score 0.673800\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[656]\tvalid_0's rmse: 1.80924\tvalid_0's l2: 3.27333\n",
            "\n",
            "===== Training model for horizon t+35 =====\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.051495 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 24944\n",
            "[LightGBM] [Info] Number of data points in the train set: 158447, number of used features: 115\n",
            "[LightGBM] [Info] Start training from score 0.673838\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[555]\tvalid_0's rmse: 1.82442\tvalid_0's l2: 3.3285\n",
            "\n",
            "===== Training model for horizon t+36 =====\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.062765 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 24943\n",
            "[LightGBM] [Info] Number of data points in the train set: 158364, number of used features: 115\n",
            "[LightGBM] [Info] Start training from score 0.673844\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[535]\tvalid_0's rmse: 1.83456\tvalid_0's l2: 3.36563\n",
            "\n",
            "===== Training model for horizon t+37 =====\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.054021 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 24943\n",
            "[LightGBM] [Info] Number of data points in the train set: 158281, number of used features: 115\n",
            "[LightGBM] [Info] Start training from score 0.673826\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[826]\tvalid_0's rmse: 1.83063\tvalid_0's l2: 3.35121\n",
            "\n",
            "===== Training model for horizon t+38 =====\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.057586 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 24943\n",
            "[LightGBM] [Info] Number of data points in the train set: 158198, number of used features: 115\n",
            "[LightGBM] [Info] Start training from score 0.673802\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[756]\tvalid_0's rmse: 1.83725\tvalid_0's l2: 3.37547\n",
            "\n",
            "===== Training model for horizon t+39 =====\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.052888 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 24943\n",
            "[LightGBM] [Info] Number of data points in the train set: 158115, number of used features: 115\n",
            "[LightGBM] [Info] Start training from score 0.673743\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[741]\tvalid_0's rmse: 1.84215\tvalid_0's l2: 3.3935\n",
            "\n",
            "===== Training model for horizon t+40 =====\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.056947 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 24943\n",
            "[LightGBM] [Info] Number of data points in the train set: 158032, number of used features: 115\n",
            "[LightGBM] [Info] Start training from score 0.673736\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[686]\tvalid_0's rmse: 1.85273\tvalid_0's l2: 3.43261\n",
            "\n",
            "===== Training model for horizon t+41 =====\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.057024 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 24944\n",
            "[LightGBM] [Info] Number of data points in the train set: 157949, number of used features: 115\n",
            "[LightGBM] [Info] Start training from score 0.673699\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[822]\tvalid_0's rmse: 1.8601\tvalid_0's l2: 3.45999\n",
            "\n",
            "===== Training model for horizon t+42 =====\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.050805 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 24945\n",
            "[LightGBM] [Info] Number of data points in the train set: 157866, number of used features: 115\n",
            "[LightGBM] [Info] Start training from score 0.673666\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[692]\tvalid_0's rmse: 1.87816\tvalid_0's l2: 3.5275\n",
            "\n",
            "===== Training model for horizon t+43 =====\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.050709 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 24945\n",
            "[LightGBM] [Info] Number of data points in the train set: 157783, number of used features: 115\n",
            "[LightGBM] [Info] Start training from score 0.673541\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[773]\tvalid_0's rmse: 1.87601\tvalid_0's l2: 3.5194\n",
            "\n",
            "===== Training model for horizon t+44 =====\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.050791 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 24945\n",
            "[LightGBM] [Info] Number of data points in the train set: 157700, number of used features: 115\n",
            "[LightGBM] [Info] Start training from score 0.673371\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[676]\tvalid_0's rmse: 1.88665\tvalid_0's l2: 3.55943\n",
            "\n",
            "===== Training model for horizon t+45 =====\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.223906 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 24944\n",
            "[LightGBM] [Info] Number of data points in the train set: 157617, number of used features: 115\n",
            "[LightGBM] [Info] Start training from score 0.673313\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[1357]\tvalid_0's rmse: 1.89053\tvalid_0's l2: 3.57409\n",
            "\n",
            "===== Training model for horizon t+46 =====\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.056043 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 24944\n",
            "[LightGBM] [Info] Number of data points in the train set: 157534, number of used features: 115\n",
            "[LightGBM] [Info] Start training from score 0.673248\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[654]\tvalid_0's rmse: 1.88981\tvalid_0's l2: 3.57138\n",
            "\n",
            "===== Training model for horizon t+47 =====\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.051427 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 24944\n",
            "[LightGBM] [Info] Number of data points in the train set: 157451, number of used features: 115\n",
            "[LightGBM] [Info] Start training from score 0.673100\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[637]\tvalid_0's rmse: 1.88675\tvalid_0's l2: 3.55984\n",
            "\n",
            "===== Training model for horizon t+48 =====\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.053018 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 24943\n",
            "[LightGBM] [Info] Number of data points in the train set: 157368, number of used features: 115\n",
            "[LightGBM] [Info] Start training from score 0.672902\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[585]\tvalid_0's rmse: 1.89181\tvalid_0's l2: 3.57895\n",
            "\n",
            "Model training complete.\n",
            "Generating and evaluating predictions on the validation set...\n",
            "\n",
            "âœ… Final Validation RMSE on original scale: 194.0529\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import lightgbm as lgb\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import gc\n",
        "\n",
        "# ===================================================================\n",
        "# 1. Load and Prepare Data\n",
        "# ===================================================================\n",
        "print(\"Loading data...\")\n",
        "df = pd.read_csv(\"/content/train.csv\")\n",
        "\n",
        "df['Timestamp'] = pd.to_datetime(df['Timestamp'])\n",
        "df = df.sort_values(by=['Location', 'Timestamp']).reset_index(drop=True)\n",
        "\n",
        "# ==> CHANGE 1: Apply log transformation to the target variable 'out'\n",
        "# We create a new column for the transformed target to keep the original for validation.\n",
        "df['out_transformed'] = np.log1p(df['out'])\n",
        "\n",
        "le = LabelEncoder()\n",
        "df['Location_encoded'] = le.fit_transform(df['Location'])\n",
        "\n",
        "\n",
        "# ===================================================================\n",
        "# 2. Feature Engineering for Time-Series\n",
        "# ===================================================================\n",
        "print(\"Engineering features...\")\n",
        "\n",
        "def create_time_series_features(df_in):\n",
        "    df_out = df_in.copy()\n",
        "    features_to_lag = ['out', 't2m', 'gust', 'cape', 'prate']\n",
        "    lag_periods = [1, 2, 3, 24, 48]\n",
        "\n",
        "    for feature in features_to_lag:\n",
        "        for lag in lag_periods:\n",
        "            df_out[f'{feature}_lag_{lag}'] = df_out.groupby('Location')[feature].shift(lag)\n",
        "\n",
        "    df_out['gust_roll_max_3h'] = df_out.groupby('Location')['gust'].transform(\n",
        "        lambda x: x.shift(1).rolling(window=3).max()\n",
        "    )\n",
        "    df_out['hour'] = df_out['Timestamp'].dt.hour\n",
        "    df_out['dayofweek'] = df_out['Timestamp'].dt.dayofweek\n",
        "    return df_out\n",
        "\n",
        "df_features = create_time_series_features(df)\n",
        "\n",
        "features = [col for col in df_features.columns if col not in [\n",
        "    'out', 'out_transformed', 'Timestamp', 'Location'\n",
        "]]\n",
        "\n",
        "\n",
        "# ===================================================================\n",
        "# 3. Create a Time-Based Validation Split\n",
        "# ===================================================================\n",
        "print(\"Creating time-based train/validation split...\")\n",
        "validation_start_date = df_features['Timestamp'].max() - pd.Timedelta(days=7)\n",
        "\n",
        "train_df = df_features[df_features['Timestamp'] < validation_start_date].copy()\n",
        "val_df = df_features[df_features['Timestamp'] >= validation_start_date].copy()\n",
        "\n",
        "train_df.dropna(inplace=True)\n",
        "\n",
        "\n",
        "# ===================================================================\n",
        "# 4. Train Models with Direct Forecasting Strategy\n",
        "# ===================================================================\n",
        "print(\"Starting model training...\")\n",
        "MAX_HORIZON = 48\n",
        "models = {}\n",
        "\n",
        "for h in range(1, MAX_HORIZON + 1):\n",
        "    print(f\"\\n===== Training model for horizon t+{h} =====\")\n",
        "\n",
        "    # Use the TRANSFORMED 'out' column as the target\n",
        "    y_train_h = train_df.groupby('Location')['out_transformed'].shift(-h)\n",
        "    X_train_h = train_df[features]\n",
        "\n",
        "    valid_indices = ~y_train_h.isna()\n",
        "    X_train_h, y_train_h = X_train_h[valid_indices], y_train_h[valid_indices]\n",
        "\n",
        "    # Create validation set for this horizon\n",
        "    X_val_h = val_df[features]\n",
        "    y_val_h = val_df.groupby('Location')['out_transformed'].shift(-h).dropna()\n",
        "    X_val_h = X_val_h.loc[y_val_h.index]\n",
        "\n",
        "    # ==> CHANGE 2: Objective is 'regression_l2' (RMSE)\n",
        "    lgbm = lgb.LGBMRegressor(\n",
        "        objective='regression_l2',\n",
        "        n_estimators=3000,\n",
        "        learning_rate=0.01,\n",
        "        num_leaves=60,\n",
        "        random_state=42,\n",
        "        n_jobs=-1,\n",
        "        colsample_bytree=0.7,\n",
        "        subsample=0.6,\n",
        "    )\n",
        "\n",
        "    lgbm.fit(\n",
        "        X_train_h, y_train_h,\n",
        "        eval_set=[(X_val_h, y_val_h)],\n",
        "        eval_metric='rmse',\n",
        "        # ==> CHANGE 3: Callback will now print the loss as it improves\n",
        "        callbacks=[lgb.early_stopping(100, verbose=True)]\n",
        "    )\n",
        "\n",
        "    models[h] = lgbm\n",
        "    gc.collect()\n",
        "\n",
        "print(\"\\nModel training complete.\")\n",
        "\n",
        "\n",
        "# ===================================================================\n",
        "# 5. Generate and Evaluate Predictions\n",
        "# ===================================================================\n",
        "print(\"Generating and evaluating predictions on the validation set...\")\n",
        "\n",
        "validation_predictions = []\n",
        "for loc_id in val_df['Location'].unique():\n",
        "    loc_df = val_df[val_df['Location'] == loc_id].copy()\n",
        "\n",
        "    # Use the features from the first timestamp of the validation set for this location\n",
        "    X_pred_loc = loc_df[features].iloc[[0]]\n",
        "\n",
        "    for h in range(1, MAX_HORIZON + 1):\n",
        "        if h > len(loc_df): continue # Cannot validate if horizon is longer than remaining data\n",
        "\n",
        "        prediction_transformed = models[h].predict(X_pred_loc)[0]\n",
        "\n",
        "        # ==> CHANGE 4: Reverse the transformation using expm1\n",
        "        prediction_original_scale = np.expm1(prediction_transformed)\n",
        "        prediction_original_scale = max(0, prediction_original_scale)\n",
        "\n",
        "        # Get the true value from the original 'out' column\n",
        "        true_value = loc_df['out'].iloc[h-1]\n",
        "\n",
        "        validation_predictions.append({\n",
        "            'pred': prediction_original_scale,\n",
        "            'true': true_value\n",
        "        })\n",
        "\n",
        "final_preds_df = pd.DataFrame(validation_predictions)\n",
        "rmse = np.sqrt(mean_squared_error(final_preds_df['true'], final_preds_df['pred']))\n",
        "print(f\"\\nâœ… Final Validation RMSE on original scale: {rmse:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KNWDUxCZQJ_i",
        "outputId": "b3c55d6e-b14a-4ce0-9751-bcb62bc5e084"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading data...\n",
            "Engineering features...\n",
            "Creating time-based train/validation split...\n",
            "Starting model training...\n",
            "\n",
            "===== Training model for horizon t+1 =====\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.059888 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 25089\n",
            "[LightGBM] [Info] Number of data points in the train set: 161269, number of used features: 116\n",
            "[LightGBM] [Info] Start training from score 0.673712\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[676]\tvalid_0's rmse: 1.10536\tvalid_0's l2: 1.22183\n",
            "\n",
            "===== Training model for horizon t+2 =====\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.060290 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 25089\n",
            "[LightGBM] [Info] Number of data points in the train set: 161186, number of used features: 116\n",
            "[LightGBM] [Info] Start training from score 0.673603\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[587]\tvalid_0's rmse: 1.21505\tvalid_0's l2: 1.47636\n",
            "\n",
            "===== Training model for horizon t+3 =====\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.062114 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 25088\n",
            "[LightGBM] [Info] Number of data points in the train set: 161103, number of used features: 116\n",
            "[LightGBM] [Info] Start training from score 0.673526\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[716]\tvalid_0's rmse: 1.29598\tvalid_0's l2: 1.67956\n",
            "\n",
            "===== Training model for horizon t+4 =====\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.259678 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 25088\n",
            "[LightGBM] [Info] Number of data points in the train set: 161020, number of used features: 116\n",
            "[LightGBM] [Info] Start training from score 0.673511\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[917]\tvalid_0's rmse: 1.34972\tvalid_0's l2: 1.82175\n",
            "\n",
            "===== Training model for horizon t+5 =====\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.055224 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 25088\n",
            "[LightGBM] [Info] Number of data points in the train set: 160937, number of used features: 116\n",
            "[LightGBM] [Info] Start training from score 0.673573\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[1076]\tvalid_0's rmse: 1.40096\tvalid_0's l2: 1.96269\n",
            "\n",
            "===== Training model for horizon t+6 =====\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.058468 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 25088\n",
            "[LightGBM] [Info] Number of data points in the train set: 160854, number of used features: 116\n",
            "[LightGBM] [Info] Start training from score 0.673632\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[1167]\tvalid_0's rmse: 1.43413\tvalid_0's l2: 2.05672\n",
            "\n",
            "===== Training model for horizon t+7 =====\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.055660 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 25089\n",
            "[LightGBM] [Info] Number of data points in the train set: 160771, number of used features: 116\n",
            "[LightGBM] [Info] Start training from score 0.673714\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[1015]\tvalid_0's rmse: 1.45933\tvalid_0's l2: 2.12964\n",
            "\n",
            "===== Training model for horizon t+8 =====\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.056912 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 25088\n",
            "[LightGBM] [Info] Number of data points in the train set: 160688, number of used features: 115\n",
            "[LightGBM] [Info] Start training from score 0.673822\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[1141]\tvalid_0's rmse: 1.48135\tvalid_0's l2: 2.1944\n",
            "\n",
            "===== Training model for horizon t+9 =====\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.053746 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 25089\n",
            "[LightGBM] [Info] Number of data points in the train set: 160605, number of used features: 115\n",
            "[LightGBM] [Info] Start training from score 0.673929\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[1581]\tvalid_0's rmse: 1.50331\tvalid_0's l2: 2.25994\n",
            "\n",
            "===== Training model for horizon t+10 =====\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.054983 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 25089\n",
            "[LightGBM] [Info] Number of data points in the train set: 160522, number of used features: 115\n",
            "[LightGBM] [Info] Start training from score 0.674038\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[1641]\tvalid_0's rmse: 1.52104\tvalid_0's l2: 2.31355\n",
            "\n",
            "===== Training model for horizon t+11 =====\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.055616 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 25089\n",
            "[LightGBM] [Info] Number of data points in the train set: 160439, number of used features: 115\n",
            "[LightGBM] [Info] Start training from score 0.674150\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[2281]\tvalid_0's rmse: 1.53875\tvalid_0's l2: 2.36775\n",
            "\n",
            "===== Training model for horizon t+12 =====\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.059653 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 25088\n",
            "[LightGBM] [Info] Number of data points in the train set: 160356, number of used features: 115\n",
            "[LightGBM] [Info] Start training from score 0.674208\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[1702]\tvalid_0's rmse: 1.5639\tvalid_0's l2: 2.44578\n",
            "\n",
            "===== Training model for horizon t+13 =====\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.058806 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 25088\n",
            "[LightGBM] [Info] Number of data points in the train set: 160273, number of used features: 115\n",
            "[LightGBM] [Info] Start training from score 0.674205\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[2446]\tvalid_0's rmse: 1.58101\tvalid_0's l2: 2.49959\n",
            "\n",
            "===== Training model for horizon t+14 =====\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.059379 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 25088\n",
            "[LightGBM] [Info] Number of data points in the train set: 160190, number of used features: 115\n",
            "[LightGBM] [Info] Start training from score 0.674162\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[2042]\tvalid_0's rmse: 1.5979\tvalid_0's l2: 2.55328\n",
            "\n",
            "===== Training model for horizon t+15 =====\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.057041 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 25088\n",
            "[LightGBM] [Info] Number of data points in the train set: 160107, number of used features: 115\n",
            "[LightGBM] [Info] Start training from score 0.674143\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[792]\tvalid_0's rmse: 1.62258\tvalid_0's l2: 2.63277\n",
            "\n",
            "===== Training model for horizon t+16 =====\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.056092 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 25088\n",
            "[LightGBM] [Info] Number of data points in the train set: 160024, number of used features: 115\n",
            "[LightGBM] [Info] Start training from score 0.674059\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[730]\tvalid_0's rmse: 1.6333\tvalid_0's l2: 2.66765\n",
            "\n",
            "===== Training model for horizon t+17 =====\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.059870 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 25088\n",
            "[LightGBM] [Info] Number of data points in the train set: 159941, number of used features: 115\n",
            "[LightGBM] [Info] Start training from score 0.673907\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[946]\tvalid_0's rmse: 1.64243\tvalid_0's l2: 2.69758\n",
            "\n",
            "===== Training model for horizon t+18 =====\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.050750 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 25088\n",
            "[LightGBM] [Info] Number of data points in the train set: 159858, number of used features: 115\n",
            "[LightGBM] [Info] Start training from score 0.673822\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[1132]\tvalid_0's rmse: 1.64196\tvalid_0's l2: 2.69602\n",
            "\n",
            "===== Training model for horizon t+19 =====\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.060033 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 25088\n",
            "[LightGBM] [Info] Number of data points in the train set: 159775, number of used features: 115\n",
            "[LightGBM] [Info] Start training from score 0.673763\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[1207]\tvalid_0's rmse: 1.65348\tvalid_0's l2: 2.73399\n",
            "\n",
            "===== Training model for horizon t+20 =====\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.055337 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 25088\n",
            "[LightGBM] [Info] Number of data points in the train set: 159692, number of used features: 115\n",
            "[LightGBM] [Info] Start training from score 0.673688\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[900]\tvalid_0's rmse: 1.65874\tvalid_0's l2: 2.75142\n",
            "\n",
            "===== Training model for horizon t+21 =====\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.042978 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 25089\n",
            "[LightGBM] [Info] Number of data points in the train set: 159609, number of used features: 115\n",
            "[LightGBM] [Info] Start training from score 0.673601\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[2261]\tvalid_0's rmse: 1.65609\tvalid_0's l2: 2.74264\n",
            "\n",
            "===== Training model for horizon t+22 =====\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.051814 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 25089\n",
            "[LightGBM] [Info] Number of data points in the train set: 159526, number of used features: 115\n",
            "[LightGBM] [Info] Start training from score 0.673524\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[2977]\tvalid_0's rmse: 1.66026\tvalid_0's l2: 2.75645\n",
            "\n",
            "===== Training model for horizon t+23 =====\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.053255 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 25089\n",
            "[LightGBM] [Info] Number of data points in the train set: 159443, number of used features: 115\n",
            "[LightGBM] [Info] Start training from score 0.673491\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[2602]\tvalid_0's rmse: 1.65704\tvalid_0's l2: 2.74577\n",
            "\n",
            "===== Training model for horizon t+24 =====\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.059542 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 25092\n",
            "[LightGBM] [Info] Number of data points in the train set: 159360, number of used features: 115\n",
            "[LightGBM] [Info] Start training from score 0.673524\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[2191]\tvalid_0's rmse: 1.67137\tvalid_0's l2: 2.79348\n",
            "\n",
            "===== Training model for horizon t+25 =====\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.051777 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 25092\n",
            "[LightGBM] [Info] Number of data points in the train set: 159277, number of used features: 115\n",
            "[LightGBM] [Info] Start training from score 0.673578\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[1611]\tvalid_0's rmse: 1.69283\tvalid_0's l2: 2.86569\n",
            "\n",
            "===== Training model for horizon t+26 =====\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.054726 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 25093\n",
            "[LightGBM] [Info] Number of data points in the train set: 159194, number of used features: 115\n",
            "[LightGBM] [Info] Start training from score 0.673674\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[3000]\tvalid_0's rmse: 1.69173\tvalid_0's l2: 2.86194\n",
            "\n",
            "===== Training model for horizon t+27 =====\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.058423 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 25091\n",
            "[LightGBM] [Info] Number of data points in the train set: 159111, number of used features: 115\n",
            "[LightGBM] [Info] Start training from score 0.673770\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[2812]\tvalid_0's rmse: 1.70931\tvalid_0's l2: 2.92172\n",
            "\n",
            "===== Training model for horizon t+28 =====\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.057065 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 25091\n",
            "[LightGBM] [Info] Number of data points in the train set: 159028, number of used features: 115\n",
            "[LightGBM] [Info] Start training from score 0.673825\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[2887]\tvalid_0's rmse: 1.72632\tvalid_0's l2: 2.98019\n",
            "\n",
            "===== Training model for horizon t+29 =====\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.040592 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 25091\n",
            "[LightGBM] [Info] Number of data points in the train set: 158945, number of used features: 115\n",
            "[LightGBM] [Info] Start training from score 0.673869\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[1407]\tvalid_0's rmse: 1.75355\tvalid_0's l2: 3.07493\n",
            "\n",
            "===== Training model for horizon t+30 =====\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.053767 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 25090\n",
            "[LightGBM] [Info] Number of data points in the train set: 158862, number of used features: 115\n",
            "[LightGBM] [Info] Start training from score 0.673883\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[817]\tvalid_0's rmse: 1.77063\tvalid_0's l2: 3.13513\n",
            "\n",
            "===== Training model for horizon t+31 =====\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.054778 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 25090\n",
            "[LightGBM] [Info] Number of data points in the train set: 158779, number of used features: 115\n",
            "[LightGBM] [Info] Start training from score 0.673875\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[981]\tvalid_0's rmse: 1.7819\tvalid_0's l2: 3.17516\n",
            "\n",
            "===== Training model for horizon t+32 =====\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.049749 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 25090\n",
            "[LightGBM] [Info] Number of data points in the train set: 158696, number of used features: 115\n",
            "[LightGBM] [Info] Start training from score 0.673844\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[676]\tvalid_0's rmse: 1.79942\tvalid_0's l2: 3.2379\n",
            "\n",
            "===== Training model for horizon t+33 =====\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.057000 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 25089\n",
            "[LightGBM] [Info] Number of data points in the train set: 158613, number of used features: 115\n",
            "[LightGBM] [Info] Start training from score 0.673792\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[848]\tvalid_0's rmse: 1.80282\tvalid_0's l2: 3.25015\n",
            "\n",
            "===== Training model for horizon t+34 =====\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.051326 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 25089\n",
            "[LightGBM] [Info] Number of data points in the train set: 158530, number of used features: 115\n",
            "[LightGBM] [Info] Start training from score 0.673800\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[600]\tvalid_0's rmse: 1.81657\tvalid_0's l2: 3.29994\n",
            "\n",
            "===== Training model for horizon t+35 =====\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.059303 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 25090\n",
            "[LightGBM] [Info] Number of data points in the train set: 158447, number of used features: 115\n",
            "[LightGBM] [Info] Start training from score 0.673838\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[548]\tvalid_0's rmse: 1.81949\tvalid_0's l2: 3.31054\n",
            "\n",
            "===== Training model for horizon t+36 =====\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.050596 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 25089\n",
            "[LightGBM] [Info] Number of data points in the train set: 158364, number of used features: 115\n",
            "[LightGBM] [Info] Start training from score 0.673844\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[519]\tvalid_0's rmse: 1.83052\tvalid_0's l2: 3.3508\n",
            "\n",
            "===== Training model for horizon t+37 =====\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.055423 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 25090\n",
            "[LightGBM] [Info] Number of data points in the train set: 158281, number of used features: 115\n",
            "[LightGBM] [Info] Start training from score 0.673826\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[835]\tvalid_0's rmse: 1.83282\tvalid_0's l2: 3.35923\n",
            "\n",
            "===== Training model for horizon t+38 =====\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.052824 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 25089\n",
            "[LightGBM] [Info] Number of data points in the train set: 158198, number of used features: 115\n",
            "[LightGBM] [Info] Start training from score 0.673802\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[889]\tvalid_0's rmse: 1.83348\tvalid_0's l2: 3.36166\n",
            "\n",
            "===== Training model for horizon t+39 =====\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.053248 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 25089\n",
            "[LightGBM] [Info] Number of data points in the train set: 158115, number of used features: 115\n",
            "[LightGBM] [Info] Start training from score 0.673743\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[596]\tvalid_0's rmse: 1.84375\tvalid_0's l2: 3.3994\n",
            "\n",
            "===== Training model for horizon t+40 =====\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.069456 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 25089\n",
            "[LightGBM] [Info] Number of data points in the train set: 158032, number of used features: 115\n",
            "[LightGBM] [Info] Start training from score 0.673736\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[653]\tvalid_0's rmse: 1.85173\tvalid_0's l2: 3.42892\n",
            "\n",
            "===== Training model for horizon t+41 =====\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.256228 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 25090\n",
            "[LightGBM] [Info] Number of data points in the train set: 157949, number of used features: 115\n",
            "[LightGBM] [Info] Start training from score 0.673699\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[674]\tvalid_0's rmse: 1.86106\tvalid_0's l2: 3.46356\n",
            "\n",
            "===== Training model for horizon t+42 =====\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.051340 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 25089\n",
            "[LightGBM] [Info] Number of data points in the train set: 157866, number of used features: 115\n",
            "[LightGBM] [Info] Start training from score 0.673666\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[638]\tvalid_0's rmse: 1.87636\tvalid_0's l2: 3.52073\n",
            "\n",
            "===== Training model for horizon t+43 =====\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.060437 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 25090\n",
            "[LightGBM] [Info] Number of data points in the train set: 157783, number of used features: 115\n",
            "[LightGBM] [Info] Start training from score 0.673541\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[690]\tvalid_0's rmse: 1.87787\tvalid_0's l2: 3.5264\n",
            "\n",
            "===== Training model for horizon t+44 =====\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.050340 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 25090\n",
            "[LightGBM] [Info] Number of data points in the train set: 157700, number of used features: 115\n",
            "[LightGBM] [Info] Start training from score 0.673371\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[734]\tvalid_0's rmse: 1.88612\tvalid_0's l2: 3.55744\n",
            "\n",
            "===== Training model for horizon t+45 =====\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.056355 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 25091\n",
            "[LightGBM] [Info] Number of data points in the train set: 157617, number of used features: 115\n",
            "[LightGBM] [Info] Start training from score 0.673313\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[910]\tvalid_0's rmse: 1.89199\tvalid_0's l2: 3.57963\n",
            "\n",
            "===== Training model for horizon t+46 =====\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.097066 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 25091\n",
            "[LightGBM] [Info] Number of data points in the train set: 157534, number of used features: 115\n",
            "[LightGBM] [Info] Start training from score 0.673248\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[633]\tvalid_0's rmse: 1.89042\tvalid_0's l2: 3.57369\n",
            "\n",
            "===== Training model for horizon t+47 =====\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.057259 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 25093\n",
            "[LightGBM] [Info] Number of data points in the train set: 157451, number of used features: 115\n",
            "[LightGBM] [Info] Start training from score 0.673100\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[615]\tvalid_0's rmse: 1.88748\tvalid_0's l2: 3.56256\n",
            "\n",
            "===== Training model for horizon t+48 =====\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.059344 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 25093\n",
            "[LightGBM] [Info] Number of data points in the train set: 157368, number of used features: 115\n",
            "[LightGBM] [Info] Start training from score 0.672902\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[629]\tvalid_0's rmse: 1.88817\tvalid_0's l2: 3.5652\n",
            "\n",
            "Model training complete.\n",
            "Generating and evaluating predictions on the validation set...\n",
            "\n",
            "âœ… Final Validation RMSE on original scale: 193.7842\n"
          ]
        }
      ]
    }
  ]
}