{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import lightgbm as lgb\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import gc\n",
        "\n",
        "# ===================================================================\n",
        "# 1. Load and Prepare Data\n",
        "# ===================================================================\n",
        "print(\"Loading data...\")\n",
        "df = pd.read_csv(\"/content/normalized_features.csv\")\n",
        "\n",
        "df['Timestamp'] = pd.to_datetime(df['Timestamp'])\n",
        "df = df.sort_values(by=['Location', 'Timestamp']).reset_index(drop=True)\n",
        "\n",
        "# ==> CHANGE 1: Apply log transformation to the target variable 'out'\n",
        "# We create a new column for the transformed target to keep the original for validation.\n",
        "df['out_transformed'] = np.log1p(df['out'])\n",
        "\n",
        "le = LabelEncoder()\n",
        "df['Location_encoded'] = le.fit_transform(df['Location'])\n",
        "\n",
        "\n",
        "# ===================================================================\n",
        "# 2. Feature Engineering for Time-Series\n",
        "# ===================================================================\n",
        "print(\"Engineering features...\")\n",
        "\n",
        "def create_time_series_features(df_in):\n",
        "    df_out = df_in.copy()\n",
        "    features_to_lag = ['out', 't2m', 'gust', 'cape', 'prate']\n",
        "    lag_periods = [1, 2, 3, 24, 48]\n",
        "\n",
        "    for feature in features_to_lag:\n",
        "        for lag in lag_periods:\n",
        "            df_out[f'{feature}_lag_{lag}'] = df_out.groupby('Location')[feature].shift(lag)\n",
        "\n",
        "    df_out['gust_roll_max_3h'] = df_out.groupby('Location')['gust'].transform(\n",
        "        lambda x: x.shift(1).rolling(window=3).max()\n",
        "    )\n",
        "    df_out['hour'] = df_out['Timestamp'].dt.hour\n",
        "    df_out['dayofweek'] = df_out['Timestamp'].dt.dayofweek\n",
        "    return df_out\n",
        "\n",
        "df_features = create_time_series_features(df)\n",
        "\n",
        "features = [col for col in df_features.columns if col not in [\n",
        "    'out', 'out_transformed', 'Timestamp', 'Location'\n",
        "]]\n",
        "\n",
        "\n",
        "# ===================================================================\n",
        "# 3. Create a Time-Based Validation Split\n",
        "# ===================================================================\n",
        "print(\"Creating time-based train/validation split...\")\n",
        "validation_start_date = df_features['Timestamp'].max() - pd.Timedelta(days=7)\n",
        "\n",
        "train_df = df_features[df_features['Timestamp'] < validation_start_date].copy()\n",
        "val_df = df_features[df_features['Timestamp'] >= validation_start_date].copy()\n",
        "\n",
        "train_df.dropna(inplace=True)\n",
        "\n",
        "\n",
        "# ===================================================================\n",
        "# 4. Train Models with Direct Forecasting Strategy\n",
        "# ===================================================================\n",
        "print(\"Starting model training...\")\n",
        "MAX_HORIZON = 48\n",
        "models = {}\n",
        "\n",
        "for h in range(1, MAX_HORIZON + 1):\n",
        "    print(f\"\\n===== Training model for horizon t+{h} =====\")\n",
        "\n",
        "    # Use the TRANSFORMED 'out' column as the target\n",
        "    y_train_h = train_df.groupby('Location')['out_transformed'].shift(-h)\n",
        "    X_train_h = train_df[features]\n",
        "\n",
        "    valid_indices = ~y_train_h.isna()\n",
        "    X_train_h, y_train_h = X_train_h[valid_indices], y_train_h[valid_indices]\n",
        "\n",
        "    # Create validation set for this horizon\n",
        "    X_val_h = val_df[features]\n",
        "    y_val_h = val_df.groupby('Location')['out_transformed'].shift(-h).dropna()\n",
        "    X_val_h = X_val_h.loc[y_val_h.index]\n",
        "\n",
        "    # ==> CHANGE 2: Objective is 'regression_l2' (RMSE)\n",
        "    lgbm = lgb.LGBMRegressor(\n",
        "        objective='regression_l2',\n",
        "        n_estimators=2000,\n",
        "        learning_rate=0.01,\n",
        "        num_leaves=40,\n",
        "        random_state=42,\n",
        "        n_jobs=-1,\n",
        "        colsample_bytree=0.7,\n",
        "        subsample=0.7,\n",
        "        device='gpu'\n",
        "    )\n",
        "\n",
        "    lgbm.fit(\n",
        "        X_train_h, y_train_h,\n",
        "        eval_set=[(X_val_h, y_val_h)],\n",
        "        eval_metric='rmse',\n",
        "        # ==> CHANGE 3: Callback will now print the loss as it improves\n",
        "        callbacks=[lgb.early_stopping(100, verbose=True)]\n",
        "    )\n",
        "\n",
        "    models[h] = lgbm\n",
        "    gc.collect()\n",
        "\n",
        "print(\"\\nModel training complete.\")\n",
        "\n",
        "\n",
        "# ===================================================================\n",
        "# 5. Generate and Evaluate Predictions\n",
        "# ===================================================================\n",
        "print(\"Generating and evaluating predictions on the validation set...\")\n",
        "\n",
        "validation_predictions = []\n",
        "for loc_id in val_df['Location'].unique():\n",
        "    loc_df = val_df[val_df['Location'] == loc_id].copy()\n",
        "\n",
        "    # Use the features from the first timestamp of the validation set for this location\n",
        "    X_pred_loc = loc_df[features].iloc[[0]]\n",
        "\n",
        "    for h in range(1, MAX_HORIZON + 1):\n",
        "        if h > len(loc_df): continue # Cannot validate if horizon is longer than remaining data\n",
        "\n",
        "        prediction_transformed = models[h].predict(X_pred_loc)[0]\n",
        "\n",
        "        # ==> CHANGE 4: Reverse the transformation using expm1\n",
        "        prediction_original_scale = np.expm1(prediction_transformed)\n",
        "        prediction_original_scale = max(0, prediction_original_scale)\n",
        "\n",
        "        # Get the true value from the original 'out' column\n",
        "        true_value = loc_df['out'].iloc[h-1]\n",
        "\n",
        "        validation_predictions.append({\n",
        "            'pred': prediction_original_scale,\n",
        "            'true': true_value\n",
        "        })\n",
        "\n",
        "final_preds_df = pd.DataFrame(validation_predictions)\n",
        "rmse = np.sqrt(mean_squared_error(final_preds_df['true'], final_preds_df['pred']))\n",
        "print(f\"\\nâœ… Final Validation RMSE on original scale: {rmse:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "_73bNaFzvxy0",
        "outputId": "72e12410-2a9d-4302-cce9-5af2c1555a42"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading data...\n",
            "Engineering features...\n",
            "Creating time-based train/validation split...\n",
            "Starting model training...\n",
            "\n",
            "===== Training model for horizon t+1 =====\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 24943\n",
            "[LightGBM] [Info] Number of data points in the train set: 161269, number of used features: 116\n",
            "[LightGBM] [Info] Using GPU Device: NVIDIA A100-SXM4-40GB, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 81 dense feature groups (12.92 MB) transferred to GPU in 0.012061 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 0.673712\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[691]\tvalid_0's rmse: 1.10076\tvalid_0's l2: 1.21166\n",
            "\n",
            "===== Training model for horizon t+2 =====\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 24943\n",
            "[LightGBM] [Info] Number of data points in the train set: 161186, number of used features: 116\n",
            "[LightGBM] [Info] Using GPU Device: NVIDIA A100-SXM4-40GB, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 81 dense feature groups (12.91 MB) transferred to GPU in 0.012836 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 0.673603\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[760]\tvalid_0's rmse: 1.2222\tvalid_0's l2: 1.49377\n",
            "\n",
            "===== Training model for horizon t+3 =====\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 24943\n",
            "[LightGBM] [Info] Number of data points in the train set: 161103, number of used features: 116\n",
            "[LightGBM] [Info] Using GPU Device: NVIDIA A100-SXM4-40GB, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 81 dense feature groups (12.91 MB) transferred to GPU in 0.012061 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 0.673526\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[606]\tvalid_0's rmse: 1.3012\tvalid_0's l2: 1.69313\n",
            "\n",
            "===== Training model for horizon t+4 =====\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 24943\n",
            "[LightGBM] [Info] Number of data points in the train set: 161020, number of used features: 116\n",
            "[LightGBM] [Info] Using GPU Device: NVIDIA A100-SXM4-40GB, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 81 dense feature groups (12.90 MB) transferred to GPU in 0.012141 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 0.673511\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[1162]\tvalid_0's rmse: 1.34898\tvalid_0's l2: 1.81974\n",
            "\n",
            "===== Training model for horizon t+5 =====\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 24944\n",
            "[LightGBM] [Info] Number of data points in the train set: 160937, number of used features: 116\n",
            "[LightGBM] [Info] Using GPU Device: NVIDIA A100-SXM4-40GB, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 81 dense feature groups (12.89 MB) transferred to GPU in 0.012431 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 0.673573\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[1179]\tvalid_0's rmse: 1.40239\tvalid_0's l2: 1.96669\n",
            "\n",
            "===== Training model for horizon t+6 =====\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 24944\n",
            "[LightGBM] [Info] Number of data points in the train set: 160854, number of used features: 116\n",
            "[LightGBM] [Info] Using GPU Device: NVIDIA A100-SXM4-40GB, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 81 dense feature groups (12.89 MB) transferred to GPU in 0.011834 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 0.673632\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[1523]\tvalid_0's rmse: 1.42874\tvalid_0's l2: 2.04131\n",
            "\n",
            "===== Training model for horizon t+7 =====\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 24945\n",
            "[LightGBM] [Info] Number of data points in the train set: 160771, number of used features: 116\n",
            "[LightGBM] [Info] Using GPU Device: NVIDIA A100-SXM4-40GB, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 81 dense feature groups (12.88 MB) transferred to GPU in 0.011881 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 0.673714\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[1334]\tvalid_0's rmse: 1.4573\tvalid_0's l2: 2.12373\n",
            "\n",
            "===== Training model for horizon t+8 =====\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 24943\n",
            "[LightGBM] [Info] Number of data points in the train set: 160688, number of used features: 115\n",
            "[LightGBM] [Info] Using GPU Device: NVIDIA A100-SXM4-40GB, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 81 dense feature groups (12.87 MB) transferred to GPU in 0.011746 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 0.673822\n",
            "Training until validation scores don't improve for 100 rounds\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3220208130.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     98\u001b[0m     )\n\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m     lgbm.fit(\n\u001b[0m\u001b[1;32m    101\u001b[0m         \u001b[0mX_train_h\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train_h\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m         \u001b[0meval_set\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_val_h\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val_h\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/lightgbm/sklearn.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, init_score, eval_set, eval_names, eval_sample_weight, eval_init_score, eval_metric, feature_name, categorical_feature, callbacks, init_model)\u001b[0m\n\u001b[1;32m   1396\u001b[0m     ) -> \"LGBMRegressor\":\n\u001b[1;32m   1397\u001b[0m         \u001b[0;34m\"\"\"Docstring is inherited from the LGBMModel.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1398\u001b[0;31m         super().fit(\n\u001b[0m\u001b[1;32m   1399\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1400\u001b[0m             \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/lightgbm/sklearn.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, init_score, group, eval_set, eval_names, eval_sample_weight, eval_class_weight, eval_init_score, eval_group, eval_metric, feature_name, categorical_feature, callbacks, init_model)\u001b[0m\n\u001b[1;32m   1047\u001b[0m         \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecord_evaluation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevals_result\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1048\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1049\u001b[0;31m         self._Booster = train(\n\u001b[0m\u001b[1;32m   1050\u001b[0m             \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1051\u001b[0m             \u001b[0mtrain_set\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_set\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/lightgbm/engine.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(params, train_set, num_boost_round, valid_sets, valid_names, feval, init_model, keep_training_booster, callbacks)\u001b[0m\n\u001b[1;32m    320\u001b[0m             )\n\u001b[1;32m    321\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 322\u001b[0;31m         \u001b[0mbooster\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfobj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    323\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    324\u001b[0m         \u001b[0mevaluation_result_list\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0m_LGBM_BoosterEvalMethodResultType\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/lightgbm/basic.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, train_set, fobj)\u001b[0m\n\u001b[1;32m   4153\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mLightGBMError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Cannot update due to null objective function.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4154\u001b[0m             _safe_call(\n\u001b[0;32m-> 4155\u001b[0;31m                 _LIB.LGBM_BoosterUpdateOneIter(\n\u001b[0m\u001b[1;32m   4156\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4157\u001b[0m                     \u001b[0mctypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbyref\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_finished\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import lightgbm as lgb\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import gc\n",
        "import joblib\n",
        "\n",
        "# ===================================================================\n",
        "# 1. Load and Prepare Data\n",
        "# ===================================================================\n",
        "print(\"Loading data...\")\n",
        "df = pd.read_csv(\"/content/normalized_features.csv\")\n",
        "df['Timestamp'] = pd.to_datetime(df['Timestamp'])\n",
        "df = df.sort_values(by=['Location', 'Timestamp']).reset_index(drop=True)\n",
        "df['out_transformed'] = np.log1p(df['out'])\n",
        "le = LabelEncoder()\n",
        "df['Location_encoded'] = le.fit_transform(df['Location'])\n",
        "\n",
        "# ===================================================================\n",
        "# 2. Feature Engineering\n",
        "# ===================================================================\n",
        "print(\"Engineering time series features...\")\n",
        "def create_time_series_features(df_in):\n",
        "    df_out = df_in.copy()\n",
        "    # Only create lags for features in your selected set\n",
        "    features_to_lag = [f for f in [\n",
        "        'out', 't2m', 'gust', 'cape', 'cape_1'\n",
        "    ] if f in df_out.columns]\n",
        "    lag_periods = [1, 2, 3, 6, 8, 10, 12, 15, 24, 48]\n",
        "    for feature in features_to_lag:\n",
        "        for lag in lag_periods:\n",
        "            df_out[f'{feature}_lag_{lag}'] = df_out.groupby('Location')[feature].shift(lag)\n",
        "\n",
        "    # Only compute rolling/std for features that exist\n",
        "    if 'gust' in df_out.columns:\n",
        "        df_out['gust_roll_max_3h'] = df_out.groupby('Location')['gust'].transform(lambda x: x.shift(1).rolling(3).max())\n",
        "        df_out['gust_std_6h'] = df_out.groupby('Location')['gust'].transform(lambda x: x.shift(1).rolling(6).std())\n",
        "        df_out['gust_acceleration_3h'] = df_out['gust_lag_1'] - df_out['gust_lag_3']\n",
        "\n",
        "    if 't2m' in df_out.columns:\n",
        "        df_out['t2m_std_12h'] = df_out.groupby('Location')['t2m'].transform(lambda x: x.shift(1).rolling(12).std())\n",
        "\n",
        "    if 'cape' in df_out.columns and 'cape_lag_1' in df_out.columns and 'cape_lag_2' in df_out.columns:\n",
        "        df_out['cape_change_1h'] = df_out['cape_lag_1'] - df_out['cape_lag_2']\n",
        "\n",
        "    df_out['hour'] = df_out['Timestamp'].dt.hour\n",
        "    df_out['dayofweek'] = df_out['Timestamp'].dt.dayofweek\n",
        "    return df_out\n",
        "\n",
        "df_features = create_time_series_features(df)\n",
        "\n",
        "# ===================================================================\n",
        "# 3. Define Final Feature List\n",
        "# ===================================================================\n",
        "features = [\n",
        "    'fricv', 'cnwat', 'max_10si', 'sdlwrf', 'slhtf', 'mslma','pt','gh_4','t', 't2m',\n",
        "    'sp', 'sulwrf','gust', 'cape', 'blh', 'sh2', 'd2m', 'lftx',  'cape_1', 'pwat'\n",
        "]\n",
        "print(f\"Training will use your selected set of {len(features)} unique features.\")\n",
        "\n",
        "# ===================================================================\n",
        "# 4. Create a Time-Based Validation Split\n",
        "# ===================================================================\n",
        "print(\"Creating time-based train/validation split...\")\n",
        "validation_start_date = df_features['Timestamp'].max() - pd.Timedelta(days=7)\n",
        "train_df = df_features[df_features['Timestamp'] < validation_start_date].copy()\n",
        "val_df = df_features[df_features['Timestamp'] >= validation_start_date].copy()\n",
        "train_df.dropna(subset=features, inplace=True)\n",
        "\n",
        "# ===================================================================\n",
        "# 5. Train Models\n",
        "# ===================================================================\n",
        "print(\"Starting model training with the selected features...\")\n",
        "\n",
        "best_params = {\n",
        "    'objective': 'regression_l2', 'metric': 'rmse', 'n_estimators': 3500,\n",
        "    'learning_rate': 0.013163, 'num_leaves': 98, 'max_depth': 18,\n",
        "    'subsample': 0.78, 'colsample_bytree': 0.65, 'reg_alpha': 0.45,\n",
        "    'reg_lambda': 0.55, 'random_state': 42, 'n_jobs': -1,\n",
        "}\n",
        "\n",
        "MAX_HORIZON = 48\n",
        "models = {}\n",
        "for h in range(1, MAX_HORIZON + 1):\n",
        "    print(f\"\\n===== Training model for horizon t+{h} =====\")\n",
        "    y_train_h = train_df.groupby('Location')['out_transformed'].shift(-h)\n",
        "    X_train_h = train_df[features]\n",
        "    valid_indices = ~y_train_h.isna()\n",
        "    X_train_h, y_train_h = X_train_h[valid_indices], y_train_h[valid_indices]\n",
        "    X_val_h = val_df[features]\n",
        "    y_val_h = val_df.groupby('Location')['out_transformed'].shift(-h).dropna()\n",
        "    X_val_h = X_val_h.loc[y_val_h.index]\n",
        "    lgbm = lgb.LGBMRegressor(**best_params)\n",
        "    lgbm.fit(\n",
        "        X_train_h, y_train_h, eval_set=[(X_val_h, y_val_h)], eval_metric='rmse',\n",
        "        callbacks=[lgb.early_stopping(100, verbose=True)]\n",
        "    )\n",
        "    models[h] = lgbm\n",
        "    gc.collect()\n",
        "\n",
        "print(\"\\nModel training complete.\")\n",
        "joblib.dump(models, 'lgbm_selected_features_models.joblib')\n",
        "print(\"âœ… Selected-feature models saved to lgbm_selected_features_models.joblib\")\n",
        "\n",
        "# ===================================================================\n",
        "# 6. Evaluate Predictions\n",
        "# ===================================================================\n",
        "print(\"Generating and evaluating predictions on the validation set...\")\n",
        "validation_predictions = []\n",
        "for loc_id in val_df['Location'].unique():\n",
        "    loc_df = val_df[val_df['Location'] == loc_id].copy()\n",
        "    X_pred_loc = loc_df[features].iloc[[0]]\n",
        "    for h in range(1, MAX_HORIZON + 1):\n",
        "        if h > len(loc_df): continue\n",
        "        prediction_transformed = models[h].predict(X_pred_loc)[0]\n",
        "        prediction_original_scale = np.expm1(prediction_transformed)\n",
        "        prediction_original_scale = max(0, prediction_original_scale)\n",
        "        true_value = loc_df['out'].iloc[h-1]\n",
        "        validation_predictions.append({'pred': prediction_original_scale, 'true': true_value})\n",
        "\n",
        "final_preds_df = pd.DataFrame(validation_predictions)\n",
        "rmse = np.sqrt(mean_squared_error(final_preds_df['true'], final_preds_df['pred']))\n",
        "print(f\"\\nâœ… Final Validation RMSE with selected features: {rmse:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8tNFiD88_eKG",
        "outputId": "a52d7364-90af-4af5-aaf7-d4c86a69e071"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading data...\n",
            "Engineering time series features...\n",
            "Training will use your selected set of 20 unique features.\n",
            "Creating time-based train/validation split...\n",
            "Starting model training with the selected features...\n",
            "\n",
            "===== Training model for horizon t+1 =====\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006734 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 4769\n",
            "[LightGBM] [Info] Number of data points in the train set: 165253, number of used features: 20\n",
            "[LightGBM] [Info] Start training from score 0.692920\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[819]\tvalid_0's rmse: 1.79206\n",
            "\n",
            "===== Training model for horizon t+2 =====\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006311 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 4769\n",
            "[LightGBM] [Info] Number of data points in the train set: 165170, number of used features: 20\n",
            "[LightGBM] [Info] Start training from score 0.692707\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[591]\tvalid_0's rmse: 1.82425\n",
            "\n",
            "===== Training model for horizon t+3 =====\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005749 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 4769\n",
            "[LightGBM] [Info] Number of data points in the train set: 165087, number of used features: 20\n",
            "[LightGBM] [Info] Start training from score 0.692360\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[521]\tvalid_0's rmse: 1.8342\n",
            "\n",
            "===== Training model for horizon t+4 =====\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006184 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 4769\n",
            "[LightGBM] [Info] Number of data points in the train set: 165004, number of used features: 20\n",
            "[LightGBM] [Info] Start training from score 0.691987\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[529]\tvalid_0's rmse: 1.84305\n",
            "\n",
            "===== Training model for horizon t+5 =====\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005846 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 4769\n",
            "[LightGBM] [Info] Number of data points in the train set: 164921, number of used features: 20\n",
            "[LightGBM] [Info] Start training from score 0.691490\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[624]\tvalid_0's rmse: 1.8466\n",
            "\n",
            "===== Training model for horizon t+6 =====\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006082 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 4769\n",
            "[LightGBM] [Info] Number of data points in the train set: 164838, number of used features: 20\n",
            "[LightGBM] [Info] Start training from score 0.690856\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[447]\tvalid_0's rmse: 1.84444\n",
            "\n",
            "===== Training model for horizon t+7 =====\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.061030 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 4769\n",
            "[LightGBM] [Info] Number of data points in the train set: 164755, number of used features: 20\n",
            "[LightGBM] [Info] Start training from score 0.690267\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[479]\tvalid_0's rmse: 1.86946\n",
            "\n",
            "===== Training model for horizon t+8 =====\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006293 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 4769\n",
            "[LightGBM] [Info] Number of data points in the train set: 164672, number of used features: 20\n",
            "[LightGBM] [Info] Start training from score 0.689652\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[445]\tvalid_0's rmse: 1.84421\n",
            "\n",
            "===== Training model for horizon t+9 =====\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004640 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 4770\n",
            "[LightGBM] [Info] Number of data points in the train set: 164589, number of used features: 20\n",
            "[LightGBM] [Info] Start training from score 0.688794\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[383]\tvalid_0's rmse: 1.84746\n",
            "\n",
            "===== Training model for horizon t+10 =====\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006357 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 4770\n",
            "[LightGBM] [Info] Number of data points in the train set: 164506, number of used features: 20\n",
            "[LightGBM] [Info] Start training from score 0.687800\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[443]\tvalid_0's rmse: 1.85279\n",
            "\n",
            "===== Training model for horizon t+11 =====\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005895 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 4770\n",
            "[LightGBM] [Info] Number of data points in the train set: 164423, number of used features: 20\n",
            "[LightGBM] [Info] Start training from score 0.686833\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[1318]\tvalid_0's rmse: 1.84931\n",
            "\n",
            "===== Training model for horizon t+12 =====\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005725 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 4770\n",
            "[LightGBM] [Info] Number of data points in the train set: 164340, number of used features: 20\n",
            "[LightGBM] [Info] Start training from score 0.685862\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[1901]\tvalid_0's rmse: 1.85037\n",
            "\n",
            "===== Training model for horizon t+13 =====\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005701 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 4770\n",
            "[LightGBM] [Info] Number of data points in the train set: 164257, number of used features: 20\n",
            "[LightGBM] [Info] Start training from score 0.684891\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "Early stopping, best iteration is:\n",
            "[1944]\tvalid_0's rmse: 1.86544\n",
            "\n",
            "===== Training model for horizon t+14 =====\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005871 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 4770\n",
            "[LightGBM] [Info] Number of data points in the train set: 164174, number of used features: 20\n",
            "[LightGBM] [Info] Start training from score 0.683953\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[1042]\tvalid_0's rmse: 1.88565\n",
            "\n",
            "===== Training model for horizon t+15 =====\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005850 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 4770\n",
            "[LightGBM] [Info] Number of data points in the train set: 164091, number of used features: 20\n",
            "[LightGBM] [Info] Start training from score 0.683094\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[1250]\tvalid_0's rmse: 1.87972\n",
            "\n",
            "===== Training model for horizon t+16 =====\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005966 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 4770\n",
            "[LightGBM] [Info] Number of data points in the train set: 164008, number of used features: 20\n",
            "[LightGBM] [Info] Start training from score 0.682288\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "Early stopping, best iteration is:\n",
            "[1918]\tvalid_0's rmse: 1.88507\n",
            "\n",
            "===== Training model for horizon t+17 =====\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005636 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 4770\n",
            "[LightGBM] [Info] Number of data points in the train set: 163925, number of used features: 20\n",
            "[LightGBM] [Info] Start training from score 0.681488\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[1805]\tvalid_0's rmse: 1.89113\n",
            "\n",
            "===== Training model for horizon t+18 =====\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006162 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 4770\n",
            "[LightGBM] [Info] Number of data points in the train set: 163842, number of used features: 20\n",
            "[LightGBM] [Info] Start training from score 0.680725\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[1404]\tvalid_0's rmse: 1.89467\n",
            "\n",
            "===== Training model for horizon t+19 =====\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005690 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 4770\n",
            "[LightGBM] [Info] Number of data points in the train set: 163759, number of used features: 20\n",
            "[LightGBM] [Info] Start training from score 0.679894\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[1246]\tvalid_0's rmse: 1.90767\n",
            "\n",
            "===== Training model for horizon t+20 =====\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.031449 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 4770\n",
            "[LightGBM] [Info] Number of data points in the train set: 163676, number of used features: 20\n",
            "[LightGBM] [Info] Start training from score 0.679060\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[1480]\tvalid_0's rmse: 1.91521\n",
            "\n",
            "===== Training model for horizon t+21 =====\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005574 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 4770\n",
            "[LightGBM] [Info] Number of data points in the train set: 163593, number of used features: 20\n",
            "[LightGBM] [Info] Start training from score 0.678396\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[1359]\tvalid_0's rmse: 1.93013\n",
            "\n",
            "===== Training model for horizon t+22 =====\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005867 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 4770\n",
            "[LightGBM] [Info] Number of data points in the train set: 163510, number of used features: 20\n",
            "[LightGBM] [Info] Start training from score 0.677881\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[1531]\tvalid_0's rmse: 1.94122\n",
            "\n",
            "===== Training model for horizon t+23 =====\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005696 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 4770\n",
            "[LightGBM] [Info] Number of data points in the train set: 163427, number of used features: 20\n",
            "[LightGBM] [Info] Start training from score 0.677367\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[1363]\tvalid_0's rmse: 1.96174\n",
            "\n",
            "===== Training model for horizon t+24 =====\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006203 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 4770\n",
            "[LightGBM] [Info] Number of data points in the train set: 163344, number of used features: 20\n",
            "[LightGBM] [Info] Start training from score 0.676962\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[1119]\tvalid_0's rmse: 1.97944\n",
            "\n",
            "===== Training model for horizon t+25 =====\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005756 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 4770\n",
            "[LightGBM] [Info] Number of data points in the train set: 163261, number of used features: 20\n",
            "[LightGBM] [Info] Start training from score 0.676647\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[891]\tvalid_0's rmse: 1.99001\n",
            "\n",
            "===== Training model for horizon t+26 =====\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005809 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 4770\n",
            "[LightGBM] [Info] Number of data points in the train set: 163178, number of used features: 20\n",
            "[LightGBM] [Info] Start training from score 0.676390\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[1227]\tvalid_0's rmse: 1.98786\n",
            "\n",
            "===== Training model for horizon t+27 =====\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005792 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 4770\n",
            "[LightGBM] [Info] Number of data points in the train set: 163095, number of used features: 20\n",
            "[LightGBM] [Info] Start training from score 0.676178\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[1010]\tvalid_0's rmse: 1.98775\n",
            "\n",
            "===== Training model for horizon t+28 =====\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005740 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 4770\n",
            "[LightGBM] [Info] Number of data points in the train set: 163012, number of used features: 20\n",
            "[LightGBM] [Info] Start training from score 0.676009\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[1085]\tvalid_0's rmse: 2.00168\n",
            "\n",
            "===== Training model for horizon t+29 =====\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005805 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 4770\n",
            "[LightGBM] [Info] Number of data points in the train set: 162929, number of used features: 20\n",
            "[LightGBM] [Info] Start training from score 0.675827\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[848]\tvalid_0's rmse: 2.01487\n",
            "\n",
            "===== Training model for horizon t+30 =====\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005863 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 4770\n",
            "[LightGBM] [Info] Number of data points in the train set: 162846, number of used features: 20\n",
            "[LightGBM] [Info] Start training from score 0.675641\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[2277]\tvalid_0's rmse: 2.02524\n",
            "\n",
            "===== Training model for horizon t+31 =====\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005787 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 4770\n",
            "[LightGBM] [Info] Number of data points in the train set: 162763, number of used features: 20\n",
            "[LightGBM] [Info] Start training from score 0.675479\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[1314]\tvalid_0's rmse: 2.02619\n",
            "\n",
            "===== Training model for horizon t+32 =====\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005870 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 4770\n",
            "[LightGBM] [Info] Number of data points in the train set: 162680, number of used features: 20\n",
            "[LightGBM] [Info] Start training from score 0.675386\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[1337]\tvalid_0's rmse: 2.04668\n",
            "\n",
            "===== Training model for horizon t+33 =====\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006065 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 4770\n",
            "[LightGBM] [Info] Number of data points in the train set: 162597, number of used features: 20\n",
            "[LightGBM] [Info] Start training from score 0.675317\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[1270]\tvalid_0's rmse: 2.05312\n",
            "\n",
            "===== Training model for horizon t+34 =====\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005795 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 4770\n",
            "[LightGBM] [Info] Number of data points in the train set: 162514, number of used features: 20\n",
            "[LightGBM] [Info] Start training from score 0.675288\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[1580]\tvalid_0's rmse: 2.06024\n",
            "\n",
            "===== Training model for horizon t+35 =====\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005997 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 4770\n",
            "[LightGBM] [Info] Number of data points in the train set: 162431, number of used features: 20\n",
            "[LightGBM] [Info] Start training from score 0.675263\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[958]\tvalid_0's rmse: 2.0779\n",
            "\n",
            "===== Training model for horizon t+36 =====\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005955 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 4770\n",
            "[LightGBM] [Info] Number of data points in the train set: 162348, number of used features: 20\n",
            "[LightGBM] [Info] Start training from score 0.675254\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[884]\tvalid_0's rmse: 2.07466\n",
            "\n",
            "===== Training model for horizon t+37 =====\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005928 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 4770\n",
            "[LightGBM] [Info] Number of data points in the train set: 162265, number of used features: 20\n",
            "[LightGBM] [Info] Start training from score 0.675221\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[1892]\tvalid_0's rmse: 2.06055\n",
            "\n",
            "===== Training model for horizon t+38 =====\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005830 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 4770\n",
            "[LightGBM] [Info] Number of data points in the train set: 162182, number of used features: 20\n",
            "[LightGBM] [Info] Start training from score 0.675127\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[2743]\tvalid_0's rmse: 2.04838\n",
            "\n",
            "===== Training model for horizon t+39 =====\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005605 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 4770\n",
            "[LightGBM] [Info] Number of data points in the train set: 162099, number of used features: 20\n",
            "[LightGBM] [Info] Start training from score 0.675005\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[1014]\tvalid_0's rmse: 2.08562\n",
            "\n",
            "===== Training model for horizon t+40 =====\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006089 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 4770\n",
            "[LightGBM] [Info] Number of data points in the train set: 162016, number of used features: 20\n",
            "[LightGBM] [Info] Start training from score 0.674873\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[1185]\tvalid_0's rmse: 2.09387\n",
            "\n",
            "===== Training model for horizon t+41 =====\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005547 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 4770\n",
            "[LightGBM] [Info] Number of data points in the train set: 161933, number of used features: 20\n",
            "[LightGBM] [Info] Start training from score 0.674756\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[926]\tvalid_0's rmse: 2.08237\n",
            "\n",
            "===== Training model for horizon t+42 =====\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007566 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 4770\n",
            "[LightGBM] [Info] Number of data points in the train set: 161850, number of used features: 20\n",
            "[LightGBM] [Info] Start training from score 0.674503\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[2084]\tvalid_0's rmse: 2.05819\n",
            "\n",
            "===== Training model for horizon t+43 =====\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005843 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 4770\n",
            "[LightGBM] [Info] Number of data points in the train set: 161767, number of used features: 20\n",
            "[LightGBM] [Info] Start training from score 0.674295\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "Early stopping, best iteration is:\n",
            "[2219]\tvalid_0's rmse: 2.04573\n",
            "\n",
            "===== Training model for horizon t+44 =====\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005562 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 4770\n",
            "[LightGBM] [Info] Number of data points in the train set: 161684, number of used features: 20\n",
            "[LightGBM] [Info] Start training from score 0.674139\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[1749]\tvalid_0's rmse: 2.04329\n",
            "\n",
            "===== Training model for horizon t+45 =====\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007751 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 4769\n",
            "[LightGBM] [Info] Number of data points in the train set: 161601, number of used features: 20\n",
            "[LightGBM] [Info] Start training from score 0.674029\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[3482]\tvalid_0's rmse: 2.02074\n",
            "\n",
            "===== Training model for horizon t+46 =====\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005632 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 4769\n",
            "[LightGBM] [Info] Number of data points in the train set: 161518, number of used features: 20\n",
            "[LightGBM] [Info] Start training from score 0.673933\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[2858]\tvalid_0's rmse: 2.02252\n",
            "\n",
            "===== Training model for horizon t+47 =====\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005863 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 4769\n",
            "[LightGBM] [Info] Number of data points in the train set: 161435, number of used features: 20\n",
            "[LightGBM] [Info] Start training from score 0.673845\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[3101]\tvalid_0's rmse: 2.04025\n",
            "\n",
            "===== Training model for horizon t+48 =====\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005716 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 4769\n",
            "[LightGBM] [Info] Number of data points in the train set: 161352, number of used features: 20\n",
            "[LightGBM] [Info] Start training from score 0.673787\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[2039]\tvalid_0's rmse: 2.0574\n",
            "\n",
            "Model training complete.\n",
            "âœ… Selected-feature models saved to lgbm_selected_features_models.joblib\n",
            "Generating and evaluating predictions on the validation set...\n",
            "\n",
            "âœ… Final Validation RMSE with selected features: 185.2180\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "second try"
      ],
      "metadata": {
        "id": "uo6UmiRlKrwG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import lightgbm as lgb\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import gc\n",
        "import joblib\n",
        "\n",
        "# ===================================================================\n",
        "# 1. Load and Prepare Data\n",
        "# ===================================================================\n",
        "print(\"Loading data...\")\n",
        "df = pd.read_csv(\"/content/normalized_features.csv\")\n",
        "df['Timestamp'] = pd.to_datetime(df['Timestamp'])\n",
        "df = df.sort_values(by=['Location', 'Timestamp']).reset_index(drop=True)\n",
        "df['out_transformed'] = np.log1p(df['out'])\n",
        "le = LabelEncoder()\n",
        "df['Location_encoded'] = le.fit_transform(df['Location'])\n",
        "\n",
        "# ===================================================================\n",
        "# 2. Feature Engineering\n",
        "# ===================================================================\n",
        "print(\"Engineering time series features...\")\n",
        "def create_time_series_features(df_in):\n",
        "    df_out = df_in.copy()\n",
        "    # Only create lags for features in your selected set\n",
        "    features_to_lag = [f for f in [\n",
        "        'out', 't2m', 'gust', 'cape', 'cape_1'\n",
        "    ] if f in df_out.columns]\n",
        "    lag_periods = [1, 2, 3, 6, 8, 10, 12, 15, 24, 48]\n",
        "    for feature in features_to_lag:\n",
        "        for lag in lag_periods:\n",
        "            df_out[f'{feature}_lag_{lag}'] = df_out.groupby('Location')[feature].shift(lag)\n",
        "\n",
        "    # Only compute rolling/std for features that exist\n",
        "    if 'gust' in df_out.columns:\n",
        "        df_out['gust_roll_max_3h'] = df_out.groupby('Location')['gust'].transform(lambda x: x.shift(1).rolling(3).max())\n",
        "        df_out['gust_std_6h'] = df_out.groupby('Location')['gust'].transform(lambda x: x.shift(1).rolling(6).std())\n",
        "        df_out['gust_acceleration_3h'] = df_out['gust_lag_1'] - df_out['gust_lag_3']\n",
        "\n",
        "    if 't2m' in df_out.columns:\n",
        "        df_out['t2m_std_12h'] = df_out.groupby('Location')['t2m'].transform(lambda x: x.shift(1).rolling(12).std())\n",
        "\n",
        "    if 'cape' in df_out.columns and 'cape_lag_1' in df_out.columns and 'cape_lag_2' in df_out.columns:\n",
        "        df_out['cape_change_1h'] = df_out['cape_lag_1'] - df_out['cape_lag_2']\n",
        "\n",
        "    df_out['hour'] = df_out['Timestamp'].dt.hour\n",
        "    df_out['dayofweek'] = df_out['Timestamp'].dt.dayofweek\n",
        "    return df_out\n",
        "\n",
        "df_features = create_time_series_features(df)\n",
        "\n",
        "# ===================================================================\n",
        "# 3. Define Final Feature List\n",
        "# ===================================================================\n",
        "features = [\n",
        "    'fricv', 'cnwat', 'max_10si', 'sdlwrf', 'slhtf', 'mslma','pt','gh_4','t', 't2m',\n",
        "    'sp', 'sulwrf','gust', 'cape', 'blh', 'sh2', 'd2m', 'lftx',  'cape_1', 'pwat'\n",
        "]\n",
        "print(f\"Training will use your selected set of {len(features)} unique features.\")\n",
        "\n",
        "# ===================================================================\n",
        "# 4. Create a Time-Based Validation Split\n",
        "# ===================================================================\n",
        "print(\"Creating time-based train/validation split...\")\n",
        "validation_start_date = df_features['Timestamp'].max() - pd.Timedelta(days=7)\n",
        "train_df = df_features[df_features['Timestamp'] < validation_start_date].copy()\n",
        "val_df = df_features[df_features['Timestamp'] >= validation_start_date].copy()\n",
        "train_df.dropna(subset=features, inplace=True)\n",
        "\n",
        "# ===================================================================\n",
        "# 5. Train Models\n",
        "# ===================================================================\n",
        "print(\"Starting model training with the selected features...\")\n",
        "\n",
        "best_params = {\n",
        "    'objective': 'regression_l2', 'metric': 'rmse', 'n_estimators': 3500,\n",
        "    'learning_rate': 0.013163, 'num_leaves': 98, 'max_depth': 18,\n",
        "    'subsample': 0.78, 'colsample_bytree': 0.65, 'reg_alpha': 0.45,\n",
        "    'reg_lambda': 0.55, 'random_state': 42, 'n_jobs': -1,\n",
        "}\n",
        "\n",
        "MAX_HORIZON = 48\n",
        "models = {}\n",
        "for h in range(1, MAX_HORIZON + 1):\n",
        "    print(f\"\\n===== Training model for horizon t+{h} =====\")\n",
        "    y_train_h = train_df.groupby('Location')['out_transformed'].shift(-h)\n",
        "    X_train_h = train_df[features]\n",
        "    valid_indices = ~y_train_h.isna()\n",
        "    X_train_h, y_train_h = X_train_h[valid_indices], y_train_h[valid_indices]\n",
        "    X_val_h = val_df[features]\n",
        "    y_val_h = val_df.groupby('Location')['out_transformed'].shift(-h).dropna()\n",
        "    X_val_h = X_val_h.loc[y_val_h.index]\n",
        "    lgbm = lgb.LGBMRegressor(**best_params)\n",
        "    lgbm.fit(\n",
        "        X_train_h, y_train_h, eval_set=[(X_val_h, y_val_h)], eval_metric='rmse',\n",
        "        callbacks=[lgb.early_stopping(100, verbose=True)]\n",
        "    )\n",
        "    models[h] = lgbm\n",
        "    gc.collect()\n",
        "\n",
        "print(\"\\nModel training complete.\")\n",
        "joblib.dump(models, 'lgbm_selected_features_models.joblib')\n",
        "print(\"âœ… Selected-feature models saved to lgbm_selected_features_models.joblib\")\n",
        "\n",
        "# ===================================================================\n",
        "# 6. Evaluate Predictions\n",
        "# ===================================================================\n",
        "print(\"Generating and evaluating predictions on the validation set...\")\n",
        "validation_predictions = []\n",
        "for loc_id in val_df['Location'].unique():\n",
        "    loc_df = val_df[val_df['Location'] == loc_id].copy()\n",
        "    X_pred_loc = loc_df[features].iloc[[0]]\n",
        "    for h in range(1, MAX_HORIZON + 1):\n",
        "        if h > len(loc_df): continue\n",
        "        prediction_transformed = models[h].predict(X_pred_loc)[0]\n",
        "        prediction_original_scale = np.expm1(prediction_transformed)\n",
        "        prediction_original_scale = max(0, prediction_original_scale)\n",
        "        true_value = loc_df['out'].iloc[h-1]\n",
        "        validation_predictions.append({'pred': prediction_original_scale, 'true': true_value})\n",
        "\n",
        "final_preds_df = pd.DataFrame(validation_predictions)\n",
        "rmse = np.sqrt(mean_squared_error(final_preds_df['true'], final_preds_df['pred']))\n",
        "print(f\"\\nâœ… Final Validation RMSE with selected features: {rmse:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0JSFcntiKrFe",
        "outputId": "f1d1ebf5-4e81-4d40-a21d-f80f7bfc39bb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading data...\n",
            "Engineering time series features...\n",
            "Training will use your selected set of 20 unique features.\n",
            "Creating time-based train/validation split...\n",
            "Starting model training with the selected features...\n",
            "\n",
            "===== Training model for horizon t+1 =====\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005732 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 4769\n",
            "[LightGBM] [Info] Number of data points in the train set: 165253, number of used features: 20\n",
            "[LightGBM] [Info] Start training from score 0.692920\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[819]\tvalid_0's rmse: 1.79206\n",
            "\n",
            "===== Training model for horizon t+2 =====\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005812 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 4769\n",
            "[LightGBM] [Info] Number of data points in the train set: 165170, number of used features: 20\n",
            "[LightGBM] [Info] Start training from score 0.692707\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[591]\tvalid_0's rmse: 1.82425\n",
            "\n",
            "===== Training model for horizon t+3 =====\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005648 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 4769\n",
            "[LightGBM] [Info] Number of data points in the train set: 165087, number of used features: 20\n",
            "[LightGBM] [Info] Start training from score 0.692360\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[521]\tvalid_0's rmse: 1.8342\n",
            "\n",
            "===== Training model for horizon t+4 =====\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.008934 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 4769\n",
            "[LightGBM] [Info] Number of data points in the train set: 165004, number of used features: 20\n",
            "[LightGBM] [Info] Start training from score 0.691987\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[529]\tvalid_0's rmse: 1.84305\n",
            "\n",
            "===== Training model for horizon t+5 =====\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005393 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 4769\n",
            "[LightGBM] [Info] Number of data points in the train set: 164921, number of used features: 20\n",
            "[LightGBM] [Info] Start training from score 0.691490\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[624]\tvalid_0's rmse: 1.8466\n",
            "\n",
            "===== Training model for horizon t+6 =====\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005801 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 4769\n",
            "[LightGBM] [Info] Number of data points in the train set: 164838, number of used features: 20\n",
            "[LightGBM] [Info] Start training from score 0.690856\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[447]\tvalid_0's rmse: 1.84444\n",
            "\n",
            "===== Training model for horizon t+7 =====\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005585 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 4769\n",
            "[LightGBM] [Info] Number of data points in the train set: 164755, number of used features: 20\n",
            "[LightGBM] [Info] Start training from score 0.690267\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[479]\tvalid_0's rmse: 1.86946\n",
            "\n",
            "===== Training model for horizon t+8 =====\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005485 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 4769\n",
            "[LightGBM] [Info] Number of data points in the train set: 164672, number of used features: 20\n",
            "[LightGBM] [Info] Start training from score 0.689652\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[445]\tvalid_0's rmse: 1.84421\n",
            "\n",
            "===== Training model for horizon t+9 =====\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.008007 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 4770\n",
            "[LightGBM] [Info] Number of data points in the train set: 164589, number of used features: 20\n",
            "[LightGBM] [Info] Start training from score 0.688794\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[383]\tvalid_0's rmse: 1.84746\n",
            "\n",
            "===== Training model for horizon t+10 =====\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007393 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 4770\n",
            "[LightGBM] [Info] Number of data points in the train set: 164506, number of used features: 20\n",
            "[LightGBM] [Info] Start training from score 0.687800\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[443]\tvalid_0's rmse: 1.85279\n",
            "\n",
            "===== Training model for horizon t+11 =====\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005571 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 4770\n",
            "[LightGBM] [Info] Number of data points in the train set: 164423, number of used features: 20\n",
            "[LightGBM] [Info] Start training from score 0.686833\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[1318]\tvalid_0's rmse: 1.84931\n",
            "\n",
            "===== Training model for horizon t+12 =====\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005836 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 4770\n",
            "[LightGBM] [Info] Number of data points in the train set: 164340, number of used features: 20\n",
            "[LightGBM] [Info] Start training from score 0.685862\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[1901]\tvalid_0's rmse: 1.85037\n",
            "\n",
            "===== Training model for horizon t+13 =====\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005532 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 4770\n",
            "[LightGBM] [Info] Number of data points in the train set: 164257, number of used features: 20\n",
            "[LightGBM] [Info] Start training from score 0.684891\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "Early stopping, best iteration is:\n",
            "[1944]\tvalid_0's rmse: 1.86544\n",
            "\n",
            "===== Training model for horizon t+14 =====\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005520 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 4770\n",
            "[LightGBM] [Info] Number of data points in the train set: 164174, number of used features: 20\n",
            "[LightGBM] [Info] Start training from score 0.683953\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[1042]\tvalid_0's rmse: 1.88565\n",
            "\n",
            "===== Training model for horizon t+15 =====\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005750 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 4770\n",
            "[LightGBM] [Info] Number of data points in the train set: 164091, number of used features: 20\n",
            "[LightGBM] [Info] Start training from score 0.683094\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[1250]\tvalid_0's rmse: 1.87972\n",
            "\n",
            "===== Training model for horizon t+16 =====\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005747 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 4770\n",
            "[LightGBM] [Info] Number of data points in the train set: 164008, number of used features: 20\n",
            "[LightGBM] [Info] Start training from score 0.682288\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "Early stopping, best iteration is:\n",
            "[1918]\tvalid_0's rmse: 1.88507\n",
            "\n",
            "===== Training model for horizon t+17 =====\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005688 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 4770\n",
            "[LightGBM] [Info] Number of data points in the train set: 163925, number of used features: 20\n",
            "[LightGBM] [Info] Start training from score 0.681488\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[1805]\tvalid_0's rmse: 1.89113\n",
            "\n",
            "===== Training model for horizon t+18 =====\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006023 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 4770\n",
            "[LightGBM] [Info] Number of data points in the train set: 163842, number of used features: 20\n",
            "[LightGBM] [Info] Start training from score 0.680725\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[1404]\tvalid_0's rmse: 1.89467\n",
            "\n",
            "===== Training model for horizon t+19 =====\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006755 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 4770\n",
            "[LightGBM] [Info] Number of data points in the train set: 163759, number of used features: 20\n",
            "[LightGBM] [Info] Start training from score 0.679894\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[1246]\tvalid_0's rmse: 1.90767\n",
            "\n",
            "===== Training model for horizon t+20 =====\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006617 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 4770\n",
            "[LightGBM] [Info] Number of data points in the train set: 163676, number of used features: 20\n",
            "[LightGBM] [Info] Start training from score 0.679060\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[1480]\tvalid_0's rmse: 1.91521\n",
            "\n",
            "===== Training model for horizon t+21 =====\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.051651 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 4770\n",
            "[LightGBM] [Info] Number of data points in the train set: 163593, number of used features: 20\n",
            "[LightGBM] [Info] Start training from score 0.678396\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[1359]\tvalid_0's rmse: 1.93013\n",
            "\n",
            "===== Training model for horizon t+22 =====\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.022472 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 4770\n",
            "[LightGBM] [Info] Number of data points in the train set: 163510, number of used features: 20\n",
            "[LightGBM] [Info] Start training from score 0.677881\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[1531]\tvalid_0's rmse: 1.94122\n",
            "\n",
            "===== Training model for horizon t+23 =====\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005478 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 4770\n",
            "[LightGBM] [Info] Number of data points in the train set: 163427, number of used features: 20\n",
            "[LightGBM] [Info] Start training from score 0.677367\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[1363]\tvalid_0's rmse: 1.96174\n",
            "\n",
            "===== Training model for horizon t+24 =====\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005588 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 4770\n",
            "[LightGBM] [Info] Number of data points in the train set: 163344, number of used features: 20\n",
            "[LightGBM] [Info] Start training from score 0.676962\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[1119]\tvalid_0's rmse: 1.97944\n",
            "\n",
            "===== Training model for horizon t+25 =====\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005629 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 4770\n",
            "[LightGBM] [Info] Number of data points in the train set: 163261, number of used features: 20\n",
            "[LightGBM] [Info] Start training from score 0.676647\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[891]\tvalid_0's rmse: 1.99001\n",
            "\n",
            "===== Training model for horizon t+26 =====\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004696 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 4770\n",
            "[LightGBM] [Info] Number of data points in the train set: 163178, number of used features: 20\n",
            "[LightGBM] [Info] Start training from score 0.676390\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[1227]\tvalid_0's rmse: 1.98786\n",
            "\n",
            "===== Training model for horizon t+27 =====\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005535 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 4770\n",
            "[LightGBM] [Info] Number of data points in the train set: 163095, number of used features: 20\n",
            "[LightGBM] [Info] Start training from score 0.676178\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[1010]\tvalid_0's rmse: 1.98775\n",
            "\n",
            "===== Training model for horizon t+28 =====\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005542 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 4770\n",
            "[LightGBM] [Info] Number of data points in the train set: 163012, number of used features: 20\n",
            "[LightGBM] [Info] Start training from score 0.676009\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[1085]\tvalid_0's rmse: 2.00168\n",
            "\n",
            "===== Training model for horizon t+29 =====\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005444 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 4770\n",
            "[LightGBM] [Info] Number of data points in the train set: 162929, number of used features: 20\n",
            "[LightGBM] [Info] Start training from score 0.675827\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[848]\tvalid_0's rmse: 2.01487\n",
            "\n",
            "===== Training model for horizon t+30 =====\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005437 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 4770\n",
            "[LightGBM] [Info] Number of data points in the train set: 162846, number of used features: 20\n",
            "[LightGBM] [Info] Start training from score 0.675641\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[2277]\tvalid_0's rmse: 2.02524\n",
            "\n",
            "===== Training model for horizon t+31 =====\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.033153 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 4770\n",
            "[LightGBM] [Info] Number of data points in the train set: 162763, number of used features: 20\n",
            "[LightGBM] [Info] Start training from score 0.675479\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[1314]\tvalid_0's rmse: 2.02619\n",
            "\n",
            "===== Training model for horizon t+32 =====\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005853 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 4770\n",
            "[LightGBM] [Info] Number of data points in the train set: 162680, number of used features: 20\n",
            "[LightGBM] [Info] Start training from score 0.675386\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[1337]\tvalid_0's rmse: 2.04668\n",
            "\n",
            "===== Training model for horizon t+33 =====\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005355 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 4770\n",
            "[LightGBM] [Info] Number of data points in the train set: 162597, number of used features: 20\n",
            "[LightGBM] [Info] Start training from score 0.675317\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[1270]\tvalid_0's rmse: 2.05312\n",
            "\n",
            "===== Training model for horizon t+34 =====\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005505 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 4770\n",
            "[LightGBM] [Info] Number of data points in the train set: 162514, number of used features: 20\n",
            "[LightGBM] [Info] Start training from score 0.675288\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[1580]\tvalid_0's rmse: 2.06024\n",
            "\n",
            "===== Training model for horizon t+35 =====\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005677 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 4770\n",
            "[LightGBM] [Info] Number of data points in the train set: 162431, number of used features: 20\n",
            "[LightGBM] [Info] Start training from score 0.675263\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[958]\tvalid_0's rmse: 2.0779\n",
            "\n",
            "===== Training model for horizon t+36 =====\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005419 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 4770\n",
            "[LightGBM] [Info] Number of data points in the train set: 162348, number of used features: 20\n",
            "[LightGBM] [Info] Start training from score 0.675254\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[884]\tvalid_0's rmse: 2.07466\n",
            "\n",
            "===== Training model for horizon t+37 =====\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005682 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 4770\n",
            "[LightGBM] [Info] Number of data points in the train set: 162265, number of used features: 20\n",
            "[LightGBM] [Info] Start training from score 0.675221\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[1892]\tvalid_0's rmse: 2.06055\n",
            "\n",
            "===== Training model for horizon t+38 =====\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005734 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 4770\n",
            "[LightGBM] [Info] Number of data points in the train set: 162182, number of used features: 20\n",
            "[LightGBM] [Info] Start training from score 0.675127\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[2743]\tvalid_0's rmse: 2.04838\n",
            "\n",
            "===== Training model for horizon t+39 =====\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005449 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 4770\n",
            "[LightGBM] [Info] Number of data points in the train set: 162099, number of used features: 20\n",
            "[LightGBM] [Info] Start training from score 0.675005\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[1014]\tvalid_0's rmse: 2.08562\n",
            "\n",
            "===== Training model for horizon t+40 =====\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005626 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 4770\n",
            "[LightGBM] [Info] Number of data points in the train set: 162016, number of used features: 20\n",
            "[LightGBM] [Info] Start training from score 0.674873\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[1185]\tvalid_0's rmse: 2.09387\n",
            "\n",
            "===== Training model for horizon t+41 =====\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005262 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 4770\n",
            "[LightGBM] [Info] Number of data points in the train set: 161933, number of used features: 20\n",
            "[LightGBM] [Info] Start training from score 0.674756\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[926]\tvalid_0's rmse: 2.08237\n",
            "\n",
            "===== Training model for horizon t+42 =====\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005336 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 4770\n",
            "[LightGBM] [Info] Number of data points in the train set: 161850, number of used features: 20\n",
            "[LightGBM] [Info] Start training from score 0.674503\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[2084]\tvalid_0's rmse: 2.05819\n",
            "\n",
            "===== Training model for horizon t+43 =====\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005671 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 4770\n",
            "[LightGBM] [Info] Number of data points in the train set: 161767, number of used features: 20\n",
            "[LightGBM] [Info] Start training from score 0.674295\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "Early stopping, best iteration is:\n",
            "[2219]\tvalid_0's rmse: 2.04573\n",
            "\n",
            "===== Training model for horizon t+44 =====\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005607 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 4770\n",
            "[LightGBM] [Info] Number of data points in the train set: 161684, number of used features: 20\n",
            "[LightGBM] [Info] Start training from score 0.674139\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[1749]\tvalid_0's rmse: 2.04329\n",
            "\n",
            "===== Training model for horizon t+45 =====\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005728 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 4769\n",
            "[LightGBM] [Info] Number of data points in the train set: 161601, number of used features: 20\n",
            "[LightGBM] [Info] Start training from score 0.674029\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[3482]\tvalid_0's rmse: 2.02074\n",
            "\n",
            "===== Training model for horizon t+46 =====\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005277 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 4769\n",
            "[LightGBM] [Info] Number of data points in the train set: 161518, number of used features: 20\n",
            "[LightGBM] [Info] Start training from score 0.673933\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[2858]\tvalid_0's rmse: 2.02252\n",
            "\n",
            "===== Training model for horizon t+47 =====\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005710 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 4769\n",
            "[LightGBM] [Info] Number of data points in the train set: 161435, number of used features: 20\n",
            "[LightGBM] [Info] Start training from score 0.673845\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[3101]\tvalid_0's rmse: 2.04025\n",
            "\n",
            "===== Training model for horizon t+48 =====\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005521 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 4769\n",
            "[LightGBM] [Info] Number of data points in the train set: 161352, number of used features: 20\n",
            "[LightGBM] [Info] Start training from score 0.673787\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[2039]\tvalid_0's rmse: 2.0574\n",
            "\n",
            "Model training complete.\n",
            "âœ… Selected-feature models saved to lgbm_selected_features_models.joblib\n",
            "Generating and evaluating predictions on the validation set...\n",
            "\n",
            "âœ… Final Validation RMSE with selected features: 185.2180\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ===================================================================\n",
        "# 4. Generate Predictions for Submission\n",
        "# ===================================================================\n",
        "print(\"Generating predictions for submission files...\")\n",
        "\n",
        "# Get the last row of data for each location from the full feature-engineered dataframe.\n",
        "# This row contains the most recent lagged features needed for prediction.\n",
        "last_known_data = df_features.groupby('Location').last().reset_index()\n",
        "\n",
        "# Load the submission templates to get the required timestamps and locations.\n",
        "sub_template_24h = pd.read_csv('/content/submission_template_24h.csv')\n",
        "sub_template_48h = pd.read_csv('/content/submission_template_48h.csv')\n",
        "\n",
        "sub_template_24h['timestamp'] = pd.to_datetime(sub_template_24h['timestamp'])\n",
        "sub_template_48h['timestamp'] = pd.to_datetime(sub_template_48h['timestamp'])\n",
        "\n",
        "\n",
        "all_predictions = []\n",
        "\n",
        "for loc_encoded, loc_id in enumerate(le.classes_):\n",
        "    # Get the feature set for the current location\n",
        "    X_pred_loc = last_known_data[last_known_data['Location'] == loc_id][features]\n",
        "\n",
        "    # If a location has no data, we can't predict\n",
        "    if X_pred_loc.empty:\n",
        "        continue\n",
        "\n",
        "    for h in range(1, MAX_HORIZON + 1):\n",
        "        # Predict using the model for horizon 'h'\n",
        "        prediction = models[h].predict(X_pred_loc)[0]\n",
        "\n",
        "        # Ensure prediction is non-negative\n",
        "        prediction = max(0, prediction)\n",
        "\n",
        "        # The prediction timestamp is h hours after the last known timestamp\n",
        "        pred_timestamp = last_known_data.loc[last_known_data['Location'] == loc_id, 'Timestamp'].iloc[0] + pd.Timedelta(hours=h)\n",
        "\n",
        "        all_predictions.append({\n",
        "            'timestamp': pred_timestamp,\n",
        "            'location': loc_id,\n",
        "            'pred': prediction\n",
        "        })\n",
        "\n",
        "predictions_df = pd.DataFrame(all_predictions)\n",
        "\n",
        "\n",
        "# ===================================================================\n",
        "# 5. Create and Save Submission Files\n",
        "# ===================================================================\n",
        "print(\"Saving submission files...\")\n",
        "\n",
        "# Create 24-hour submission file\n",
        "submission_24h = sub_template_24h[['timestamp', 'location']].copy()\n",
        "submission_24h = submission_24h.merge(predictions_df, on=['timestamp', 'location'], how='left')\n",
        "submission_24h['pred'] = submission_24h['pred'].fillna(0) # Fill any misses with 0\n",
        "\n",
        "# Create 48-hour submission file\n",
        "submission_48h = sub_template_48h[['timestamp', 'location']].copy()\n",
        "submission_48h = submission_48h.merge(predictions_df, on=['timestamp', 'location'], how='left')\n",
        "submission_48h['pred'] = submission_48h['pred'].fillna(0) # Fill any misses with 0\n",
        "\n",
        "\n",
        "# Save to CSV in the specified format without the index\n",
        "submission_24h.to_csv('submission_24h.csv', index=False)\n",
        "submission_48h.to_csv('submission_48h.csv', index=False)\n",
        "\n",
        "print(f\"âœ… Successfully created submission_24h.csv and submission_48h.csv\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oIkZwzZq7WD4",
        "outputId": "9a867927-cf58-4d9e-8138-73a72c3c3602"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating predictions for submission files...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-269123749.py:14: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
            "  sub_template_24h['timestamp'] = pd.to_datetime(sub_template_24h['timestamp'])\n",
            "/tmp/ipython-input-269123749.py:15: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
            "  sub_template_48h['timestamp'] = pd.to_datetime(sub_template_48h['timestamp'])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving submission files...\n",
            "âœ… Successfully created submission_24h.csv and submission_48h.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib\n",
        "\n",
        "joblib.dump(models, 'lgbm_direct_forecasting_models.joblib')\n",
        "print(\"âœ… Models have been saved to lgbm_direct_forecasting_models.joblib\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AJACf4qY8T8P",
        "outputId": "e2749977-bfe9-43e2-80c9-e49411b7fb67"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Models have been saved to lgbm_direct_forecasting_models.joblib\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "TRYING TO IMPROVE RMSE EVEN FURTHER"
      ],
      "metadata": {
        "id": "CV5Fi8EV_yZe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import lightgbm as lgb\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import gc\n",
        "\n",
        "# ===================================================================\n",
        "# 1. Load and Prepare Data\n",
        "# ===================================================================\n",
        "print(\"Loading data...\")\n",
        "df = pd.read_csv(\"/content/normalized_features.csv\")\n",
        "\n",
        "df['Timestamp'] = pd.to_datetime(df['Timestamp'])\n",
        "df = df.sort_values(by=['Location', 'Timestamp']).reset_index(drop=True)\n",
        "\n",
        "# ==> CHANGE 1: Apply log transformation to the target variable 'out'\n",
        "# We create a new column for the transformed target to keep the original for validation.\n",
        "df['out_transformed'] = np.log1p(df['out'])\n",
        "\n",
        "le = LabelEncoder()\n",
        "df['Location_encoded'] = le.fit_transform(df['Location'])\n",
        "\n",
        "\n",
        "# ===================================================================\n",
        "# 2. Feature Engineering for Time-Series\n",
        "# ===================================================================\n",
        "print(\"Engineering features...\")\n",
        "\n",
        "def create_time_series_features(df_in):\n",
        "    df_out = df_in.copy()\n",
        "    features_to_lag = ['out', 't2m', 'gust', 'cape', 'prate', 'fricv']\n",
        "    lag_periods = [1, 2, 3, 24, 48]\n",
        "\n",
        "    for feature in features_to_lag:\n",
        "        for lag in lag_periods:\n",
        "            df_out[f'{feature}_lag_{lag}'] = df_out.groupby('Location')[feature].shift(lag)\n",
        "\n",
        "    df_out['gust_roll_max_3h'] = df_out.groupby('Location')['gust'].transform(\n",
        "        lambda x: x.shift(1).rolling(window=3).max()\n",
        "    )\n",
        "    df_out['hour'] = df_out['Timestamp'].dt.hour\n",
        "    df_out['dayofweek'] = df_out['Timestamp'].dt.dayofweek\n",
        "    return df_out\n",
        "\n",
        "df_features = create_time_series_features(df)\n",
        "\n",
        "features = [col for col in df_features.columns if col not in [\n",
        "    'out', 'out_transformed', 'Timestamp', 'Location'\n",
        "]]\n",
        "\n",
        "\n",
        "# ===================================================================\n",
        "# 3. Create a Time-Based Validation Split\n",
        "# ===================================================================\n",
        "print(\"Creating time-based train/validation split...\")\n",
        "validation_start_date = df_features['Timestamp'].max() - pd.Timedelta(days=7)\n",
        "\n",
        "train_df = df_features[df_features['Timestamp'] < validation_start_date].copy()\n",
        "val_df = df_features[df_features['Timestamp'] >= validation_start_date].copy()\n",
        "\n",
        "train_df.dropna(inplace=True)\n",
        "\n",
        "\n",
        "# ===================================================================\n",
        "# 4. Train Models with Direct Forecasting Strategy\n",
        "# ===================================================================\n",
        "print(\"Starting model training...\")\n",
        "MAX_HORIZON = 48\n",
        "models = {}\n",
        "\n",
        "for h in range(1, MAX_HORIZON + 1):\n",
        "    print(f\"\\n===== Training model for horizon t+{h} =====\")\n",
        "\n",
        "    # Use the TRANSFORMED 'out' column as the target\n",
        "    y_train_h = train_df.groupby('Location')['out_transformed'].shift(-h)\n",
        "    X_train_h = train_df[features]\n",
        "\n",
        "    valid_indices = ~y_train_h.isna()\n",
        "    X_train_h, y_train_h = X_train_h[valid_indices], y_train_h[valid_indices]\n",
        "\n",
        "    # Create validation set for this horizon\n",
        "    X_val_h = val_df[features]\n",
        "    y_val_h = val_df.groupby('Location')['out_transformed'].shift(-h).dropna()\n",
        "    X_val_h = X_val_h.loc[y_val_h.index]\n",
        "\n",
        "    # ==> CHANGE 2: Objective is 'regression_l2' (RMSE)\n",
        "    lgbm = lgb.LGBMRegressor(\n",
        "        objective='regression_l2',\n",
        "        n_estimators=3000,\n",
        "        learning_rate=0.01,\n",
        "        num_leaves=60,\n",
        "        random_state=42,\n",
        "        n_jobs=-1,\n",
        "        colsample_bytree=0.7,\n",
        "        subsample=0.6,\n",
        "    )\n",
        "\n",
        "    lgbm.fit(\n",
        "        X_train_h, y_train_h,\n",
        "        eval_set=[(X_val_h, y_val_h)],\n",
        "        eval_metric='rmse',\n",
        "        # ==> CHANGE 3: Callback will now print the loss as it improves\n",
        "        callbacks=[lgb.early_stopping(100, verbose=True)]\n",
        "    )\n",
        "\n",
        "    models[h] = lgbm\n",
        "    gc.collect()\n",
        "\n",
        "print(\"\\nModel training complete.\")\n",
        "\n",
        "\n",
        "# ===================================================================\n",
        "# 5. Generate and Evaluate Predictions\n",
        "# ===================================================================\n",
        "print(\"Generating and evaluating predictions on the validation set...\")\n",
        "\n",
        "validation_predictions = []\n",
        "for loc_id in val_df['Location'].unique():\n",
        "    loc_df = val_df[val_df['Location'] == loc_id].copy()\n",
        "\n",
        "    # Use the features from the first timestamp of the validation set for this location\n",
        "    X_pred_loc = loc_df[features].iloc[[0]]\n",
        "\n",
        "    for h in range(1, MAX_HORIZON + 1):\n",
        "        if h > len(loc_df): continue # Cannot validate if horizon is longer than remaining data\n",
        "\n",
        "        prediction_transformed = models[h].predict(X_pred_loc)[0]\n",
        "\n",
        "        # ==> CHANGE 4: Reverse the transformation using expm1\n",
        "        prediction_original_scale = np.expm1(prediction_transformed)\n",
        "        prediction_original_scale = max(0, prediction_original_scale)\n",
        "\n",
        "        # Get the true value from the original 'out' column\n",
        "        true_value = loc_df['out'].iloc[h-1]\n",
        "\n",
        "        validation_predictions.append({\n",
        "            'pred': prediction_original_scale,\n",
        "            'true': true_value\n",
        "        })\n",
        "\n",
        "final_preds_df = pd.DataFrame(validation_predictions)\n",
        "rmse = np.sqrt(mean_squared_error(final_preds_df['true'], final_preds_df['pred']))\n",
        "print(f\"\\nâœ… Final Validation RMSE on original scale: {rmse:.4f}\")"
      ],
      "metadata": {
        "id": "I2n232ZU8_bg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import lightgbm as lgb\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import gc\n",
        "\n",
        "# ===================================================================\n",
        "# 1. Load and Prepare Data\n",
        "# ===================================================================\n",
        "print(\"Loading data...\")\n",
        "df = pd.read_csv(\"/content/train.csv\")\n",
        "\n",
        "df['Timestamp'] = pd.to_datetime(df['Timestamp'])\n",
        "df = df.sort_values(by=['Location', 'Timestamp']).reset_index(drop=True)\n",
        "\n",
        "# ==> CHANGE 1: Apply log transformation to the target variable 'out'\n",
        "# We create a new column for the transformed target to keep the original for validation.\n",
        "df['out_transformed'] = np.log1p(df['out'])\n",
        "\n",
        "le = LabelEncoder()\n",
        "df['Location_encoded'] = le.fit_transform(df['Location'])\n",
        "\n",
        "\n",
        "# ===================================================================\n",
        "# 2. Feature Engineering for Time-Series\n",
        "# ===================================================================\n",
        "print(\"Engineering features...\")\n",
        "\n",
        "def create_time_series_features(df_in):\n",
        "    df_out = df_in.copy()\n",
        "    features_to_lag = ['out', 't2m', 'gust', 'cape', 'prate']\n",
        "    lag_periods = [1, 2, 3, 24, 48]\n",
        "\n",
        "    for feature in features_to_lag:\n",
        "        for lag in lag_periods:\n",
        "            df_out[f'{feature}_lag_{lag}'] = df_out.groupby('Location')[feature].shift(lag)\n",
        "\n",
        "    df_out['gust_roll_max_3h'] = df_out.groupby('Location')['gust'].transform(\n",
        "        lambda x: x.shift(1).rolling(window=3).max()\n",
        "    )\n",
        "    df_out['hour'] = df_out['Timestamp'].dt.hour\n",
        "    df_out['dayofweek'] = df_out['Timestamp'].dt.dayofweek\n",
        "    return df_out\n",
        "\n",
        "df_features = create_time_series_features(df)\n",
        "\n",
        "features = [col for col in df_features.columns if col not in [\n",
        "    'out', 'out_transformed', 'Timestamp', 'Location'\n",
        "]]\n",
        "\n",
        "\n",
        "# ===================================================================\n",
        "# 3. Create a Time-Based Validation Split\n",
        "# ===================================================================\n",
        "print(\"Creating time-based train/validation split...\")\n",
        "validation_start_date = df_features['Timestamp'].max() - pd.Timedelta(days=7)\n",
        "\n",
        "train_df = df_features[df_features['Timestamp'] < validation_start_date].copy()\n",
        "val_df = df_features[df_features['Timestamp'] >= validation_start_date].copy()\n",
        "\n",
        "train_df.dropna(inplace=True)\n",
        "\n",
        "\n",
        "# ===================================================================\n",
        "# 4. Train Models with Direct Forecasting Strategy\n",
        "# ===================================================================\n",
        "print(\"Starting model training...\")\n",
        "MAX_HORIZON = 48\n",
        "models = {}\n",
        "\n",
        "for h in range(1, MAX_HORIZON + 1):\n",
        "    print(f\"\\n===== Training model for horizon t+{h} =====\")\n",
        "\n",
        "    # Use the TRANSFORMED 'out' column as the target\n",
        "    y_train_h = train_df.groupby('Location')['out_transformed'].shift(-h)\n",
        "    X_train_h = train_df[features]\n",
        "\n",
        "    valid_indices = ~y_train_h.isna()\n",
        "    X_train_h, y_train_h = X_train_h[valid_indices], y_train_h[valid_indices]\n",
        "\n",
        "    # Create validation set for this horizon\n",
        "    X_val_h = val_df[features]\n",
        "    y_val_h = val_df.groupby('Location')['out_transformed'].shift(-h).dropna()\n",
        "    X_val_h = X_val_h.loc[y_val_h.index]\n",
        "\n",
        "    # ==> CHANGE 2: Objective is 'regression_l2' (RMSE)\n",
        "    lgbm = lgb.LGBMRegressor(\n",
        "        objective='regression_l2',\n",
        "        n_estimators=3000,\n",
        "        learning_rate=0.01,\n",
        "        num_leaves=60,\n",
        "        random_state=42,\n",
        "        n_jobs=-1,\n",
        "        colsample_bytree=0.7,\n",
        "        subsample=0.6,\n",
        "    )\n",
        "\n",
        "    lgbm.fit(\n",
        "        X_train_h, y_train_h,\n",
        "        eval_set=[(X_val_h, y_val_h)],\n",
        "        eval_metric='rmse',\n",
        "        # ==> CHANGE 3: Callback will now print the loss as it improves\n",
        "        callbacks=[lgb.early_stopping(100, verbose=True)]\n",
        "    )\n",
        "\n",
        "    models[h] = lgbm\n",
        "    gc.collect()\n",
        "\n",
        "print(\"\\nModel training complete.\")\n",
        "\n",
        "\n",
        "# ===================================================================\n",
        "# 5. Generate and Evaluate Predictions\n",
        "# ===================================================================\n",
        "print(\"Generating and evaluating predictions on the validation set...\")\n",
        "\n",
        "validation_predictions = []\n",
        "for loc_id in val_df['Location'].unique():\n",
        "    loc_df = val_df[val_df['Location'] == loc_id].copy()\n",
        "\n",
        "    # Use the features from the first timestamp of the validation set for this location\n",
        "    X_pred_loc = loc_df[features].iloc[[0]]\n",
        "\n",
        "    for h in range(1, MAX_HORIZON + 1):\n",
        "        if h > len(loc_df): continue # Cannot validate if horizon is longer than remaining data\n",
        "\n",
        "        prediction_transformed = models[h].predict(X_pred_loc)[0]\n",
        "\n",
        "        # ==> CHANGE 4: Reverse the transformation using expm1\n",
        "        prediction_original_scale = np.expm1(prediction_transformed)\n",
        "        prediction_original_scale = max(0, prediction_original_scale)\n",
        "\n",
        "        # Get the true value from the original 'out' column\n",
        "        true_value = loc_df['out'].iloc[h-1]\n",
        "\n",
        "        validation_predictions.append({\n",
        "            'pred': prediction_original_scale,\n",
        "            'true': true_value\n",
        "        })\n",
        "\n",
        "final_preds_df = pd.DataFrame(validation_predictions)\n",
        "rmse = np.sqrt(mean_squared_error(final_preds_df['true'], final_preds_df['pred']))\n",
        "print(f\"\\nâœ… Final Validation RMSE on original scale: {rmse:.4f}\")"
      ],
      "metadata": {
        "id": "KNWDUxCZQJ_i"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}